{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-A-summary_seq-to-seq-attention.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V3KS-AVSKyR",
        "outputId": "521a4f30-06d2-493a-8759-50c398c10831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AutoMaster_TrainSet.csv',\n",
              " 'AutoMaster_TestSet.csv',\n",
              " 'stop',\n",
              " 'save_embedding_matrix_path',\n",
              " 'train_x_pad.txt',\n",
              " 'train_y_pad.txt',\n",
              " 'test_x_pad.txt',\n",
              " 'train_x_pad.txt.npy',\n",
              " 'embedding_matrix.txt.npy',\n",
              " 'merged_train_test_seg_data.csv',\n",
              " 'word2vec.model',\n",
              " 'wordcloud.png',\n",
              " 'train_x_pad',\n",
              " 'train_y_pad',\n",
              " 'test_x_pad',\n",
              " 'new_word2vec_model',\n",
              " 'embedding_matrix.txt',\n",
              " 'vocab.json',\n",
              " 'reverse_vocab.json']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive/Q-A summary/\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ECyPNBDtSRzU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_path='embedding_matrix.txt'"
      ],
      "metadata": {
        "id": "uQFlvOchSTE2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines=[]\n",
        "with open(embedding_matrix_path) as f:\n",
        "    for line in f:\n",
        "        l=line.split(\" \")\n",
        "        l=[float(i) for i in l]\n",
        "        lines.append(l)\n",
        "np.save(embedding_matrix_path,lines)"
      ],
      "metadata": {
        "id": "qTMqzN3WSUOU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix=np.array(lines)"
      ],
      "metadata": {
        "id": "FWTRhcsvSVfX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbXKi-9GSW71",
        "outputId": "666ae405-ada1-498a-ee03-b8a3c6814879"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31937, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path):\n",
        "  lines=[]\n",
        "  with open(path) as f:\n",
        "    for line in f:\n",
        "        l=line.split(\" \")\n",
        "        l=[int(float(i)) for i in l]\n",
        "        lines.append(l)\n",
        "  return np.array(lines)\n",
        "\n",
        "train_x=read_data(\"train_x_pad.txt\")\n",
        "train_y=read_data(\"train_y_pad.txt\")\n",
        "test_x=read_data(\"test_x_pad.txt\")"
      ],
      "metadata": {
        "id": "TSG_uxEeSYc5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"vocab.json\",'r', encoding='UTF-8') as f:\n",
        "     vocab = json.load(f)"
      ],
      "metadata": {
        "id": "FAt2xj8XcdbL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"reverse_vocab.json\",'r', encoding='UTF-8') as f:\n",
        "     reverse_vocab = json.load(f)"
      ],
      "metadata": {
        "id": "GJejDUUJjkja"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq-to-Seq Model with attention"
      ],
      "metadata": {
        "id": "MXvpA3fYczes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "metadata": {
        "id": "31Jbnquvcvb1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练集的长度\n",
        "BUFFER_SIZE = len(train_x)\n",
        " \n",
        "# 输入的长度\n",
        "max_length_inp=train_x.shape[1]\n",
        "# 输出的长度\n",
        "max_length_targ=train_y.shape[1]\n",
        " \n",
        "BATCH_SIZE = 64\n",
        " \n",
        "# 训练一轮需要迭代多少步\n",
        "steps_per_epoch = len(train_x)//BATCH_SIZE\n",
        " \n",
        "# 词向量维度\n",
        "embedding_dim = 200\n",
        "# 隐藏层单元数\n",
        "units = 1024\n",
        " \n",
        "# 词表大小\n",
        "#vocab_size = len(vocab)\n",
        "\n",
        "vocab_size = 31937\n",
        "# 构建训练集\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "EP4S4KfbqmLQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim ,embedding_matrix , enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],trainable=False)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        " \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        " \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "RPg8HPDIqXmd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim,embedding_matrix, units, BATCH_SIZE)\n",
        "# example_input\n",
        "example_input_batch = tf.ones(shape=(BATCH_SIZE,max_length_inp), dtype=tf.int32)\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spb8UyRwqYkU",
        "outputId": "512e6f8e-5b60-4ad2-988c-5b5a37dd7113"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 415, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        " \n",
        "    def call(self, query, values):\n",
        "        \n",
        "        # query为上次的GRU隐藏层\n",
        "        # values为编码器的编码结果enc_output\n",
        "        # 在seq2seq模型中，St是后面的query向量，而编码过程的隐藏状态hi是values。\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        " \n",
        "        \n",
        "        # 计算注意力权重值\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        " \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # # 使用注意力权重*编码器输出作为返回值，将来会作为解码器的输入\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        " \n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "vh3fuJh7r5yk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim,embedding_matrix, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[embedding_matrix],trainable=False)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        " \n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        " \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # 使用上次的隐藏层（第一次使用编码器隐藏层）、编码器输出计算注意力权重\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        " \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # 将上一循环的预测结果跟注意力权重值结合在一起作为本次的GRU网络输入\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        " \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        " \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        " \n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        " \n",
        "        return x, state, attention_weights\n"
      ],
      "metadata": {
        "id": "MnxG42xRcG7l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, embedding_dim,embedding_matrix, units, BATCH_SIZE)\n",
        " \n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        " \n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsfxBK1Bj2Cs",
        "outputId": "0320b3fc-d9dc-4cc8-fb4e-c1df35d6f9fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 31937)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        " \n",
        "pad_index=vocab['<PAD>']\n",
        " \n",
        "def loss_function(real, pred):\n",
        "    #找到非<pad>对应的位置\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, pad_index))\n",
        "    \n",
        "    loss_ = loss_object(real, pred)\n",
        " \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    #排除<pad>的loss\n",
        "    loss_ *= mask\n",
        " \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n"
      ],
      "metadata": {
        "id": "zkhlcra4cZOL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = 'data/checkpoints/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ],
      "metadata": {
        "id": "DfeYp_afjwQB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        " \n",
        "    with tf.GradientTape() as tape:\n",
        "        # 1. 构建encoder\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        # 2. 复制\n",
        "        dec_hidden = enc_hidden\n",
        "        # 3. <START> * BATCH_SIZE \n",
        "        dec_input = tf.expand_dims([vocab['<START>']] * BATCH_SIZE, 1)\n",
        " \n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # decoder(x, hidden, enc_output)\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            \n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        " \n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        " \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        " \n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        " \n",
        "        gradients = tape.gradient(loss, variables)\n",
        " \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        " \n",
        "        return batch_loss"
      ],
      "metadata": {
        "id": "SwBWN6mVj6xP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        " \n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # 初始化隐藏层\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        " \n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        # \n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        " \n",
        "        if batch % 1 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        " \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoyVhytQkLeo",
        "outputId": "349eaa94-7cdf-4873-8181-958970fc467d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 3.9419\n",
            "Epoch 1 Batch 1 Loss 4.0303\n",
            "Epoch 1 Batch 2 Loss 4.1316\n",
            "Epoch 1 Batch 3 Loss 4.1997\n",
            "Epoch 1 Batch 4 Loss 4.1552\n",
            "Epoch 1 Batch 5 Loss 3.9734\n",
            "Epoch 1 Batch 6 Loss 2.8997\n",
            "Epoch 1 Batch 7 Loss 3.3855\n",
            "Epoch 1 Batch 8 Loss 2.6335\n",
            "Epoch 1 Batch 9 Loss 2.7535\n",
            "Epoch 1 Batch 10 Loss 2.7068\n",
            "Epoch 1 Batch 11 Loss 2.9738\n",
            "Epoch 1 Batch 12 Loss 2.8820\n",
            "Epoch 1 Batch 13 Loss 2.8849\n",
            "Epoch 1 Batch 14 Loss 3.0601\n",
            "Epoch 1 Batch 15 Loss 2.8586\n",
            "Epoch 1 Batch 16 Loss 2.8794\n",
            "Epoch 1 Batch 17 Loss 2.8129\n",
            "Epoch 1 Batch 18 Loss 2.8019\n",
            "Epoch 1 Batch 19 Loss 2.9293\n",
            "Epoch 1 Batch 20 Loss 2.5359\n",
            "Epoch 1 Batch 21 Loss 3.0643\n",
            "Epoch 1 Batch 22 Loss 2.9162\n",
            "Epoch 1 Batch 23 Loss 2.6880\n",
            "Epoch 1 Batch 24 Loss 2.8615\n",
            "Epoch 1 Batch 25 Loss 2.7165\n",
            "Epoch 1 Batch 26 Loss 2.7638\n",
            "Epoch 1 Batch 27 Loss 2.5627\n",
            "Epoch 1 Batch 28 Loss 2.7990\n",
            "Epoch 1 Batch 29 Loss 2.9568\n",
            "Epoch 1 Batch 30 Loss 2.8978\n",
            "Epoch 1 Batch 31 Loss 3.2291\n",
            "Epoch 1 Batch 32 Loss 2.6036\n",
            "Epoch 1 Batch 33 Loss 2.7034\n",
            "Epoch 1 Batch 34 Loss 2.9183\n",
            "Epoch 1 Batch 35 Loss 2.5359\n",
            "Epoch 1 Batch 36 Loss 3.0410\n",
            "Epoch 1 Batch 37 Loss 2.9727\n",
            "Epoch 1 Batch 38 Loss 2.9302\n",
            "Epoch 1 Batch 39 Loss 2.6471\n",
            "Epoch 1 Batch 40 Loss 2.5753\n",
            "Epoch 1 Batch 41 Loss 2.6474\n",
            "Epoch 1 Batch 42 Loss 2.9789\n",
            "Epoch 1 Batch 43 Loss 2.5117\n",
            "Epoch 1 Batch 44 Loss 2.4449\n",
            "Epoch 1 Batch 45 Loss 2.5182\n",
            "Epoch 1 Batch 46 Loss 2.8565\n",
            "Epoch 1 Batch 47 Loss 2.9938\n",
            "Epoch 1 Batch 48 Loss 2.7780\n",
            "Epoch 1 Batch 49 Loss 2.6222\n",
            "Epoch 1 Batch 50 Loss 2.5235\n",
            "Epoch 1 Batch 51 Loss 3.0164\n",
            "Epoch 1 Batch 52 Loss 2.1989\n",
            "Epoch 1 Batch 53 Loss 2.6025\n",
            "Epoch 1 Batch 54 Loss 2.7156\n",
            "Epoch 1 Batch 55 Loss 2.8392\n",
            "Epoch 1 Batch 56 Loss 2.6177\n",
            "Epoch 1 Batch 57 Loss 2.5209\n",
            "Epoch 1 Batch 58 Loss 2.5428\n",
            "Epoch 1 Batch 59 Loss 2.7995\n",
            "Epoch 1 Batch 60 Loss 2.7400\n",
            "Epoch 1 Batch 61 Loss 2.8004\n",
            "Epoch 1 Batch 62 Loss 2.3698\n",
            "Epoch 1 Batch 63 Loss 2.5504\n",
            "Epoch 1 Batch 64 Loss 2.7034\n",
            "Epoch 1 Batch 65 Loss 2.7266\n",
            "Epoch 1 Batch 66 Loss 2.5392\n",
            "Epoch 1 Batch 67 Loss 2.8267\n",
            "Epoch 1 Batch 68 Loss 2.7600\n",
            "Epoch 1 Batch 69 Loss 2.3337\n",
            "Epoch 1 Batch 70 Loss 2.7571\n",
            "Epoch 1 Batch 71 Loss 2.7708\n",
            "Epoch 1 Batch 72 Loss 2.5411\n",
            "Epoch 1 Batch 73 Loss 2.3100\n",
            "Epoch 1 Batch 74 Loss 2.7264\n",
            "Epoch 1 Batch 75 Loss 2.9574\n",
            "Epoch 1 Batch 76 Loss 2.7832\n",
            "Epoch 1 Batch 77 Loss 2.8066\n",
            "Epoch 1 Batch 78 Loss 2.4772\n",
            "Epoch 1 Batch 79 Loss 2.5753\n",
            "Epoch 1 Batch 80 Loss 2.7507\n",
            "Epoch 1 Batch 81 Loss 2.4000\n",
            "Epoch 1 Batch 82 Loss 2.4161\n",
            "Epoch 1 Batch 83 Loss 2.4737\n",
            "Epoch 1 Batch 84 Loss 2.6212\n",
            "Epoch 1 Batch 85 Loss 2.3878\n",
            "Epoch 1 Batch 86 Loss 2.7046\n",
            "Epoch 1 Batch 87 Loss 2.6556\n",
            "Epoch 1 Batch 88 Loss 2.6228\n",
            "Epoch 1 Batch 89 Loss 2.5445\n",
            "Epoch 1 Batch 90 Loss 2.4048\n",
            "Epoch 1 Batch 91 Loss 2.4003\n",
            "Epoch 1 Batch 92 Loss 2.1574\n",
            "Epoch 1 Batch 93 Loss 2.2170\n",
            "Epoch 1 Batch 94 Loss 2.4004\n",
            "Epoch 1 Batch 95 Loss 2.7976\n",
            "Epoch 1 Batch 96 Loss 2.1631\n",
            "Epoch 1 Batch 97 Loss 2.5003\n",
            "Epoch 1 Batch 98 Loss 2.0284\n",
            "Epoch 1 Batch 99 Loss 2.6627\n",
            "Epoch 1 Batch 100 Loss 2.5619\n",
            "Epoch 1 Batch 101 Loss 2.3925\n",
            "Epoch 1 Batch 102 Loss 2.3029\n",
            "Epoch 1 Batch 103 Loss 2.1237\n",
            "Epoch 1 Batch 104 Loss 2.5675\n",
            "Epoch 1 Batch 105 Loss 2.2569\n",
            "Epoch 1 Batch 106 Loss 2.2349\n",
            "Epoch 1 Batch 107 Loss 2.3009\n",
            "Epoch 1 Batch 108 Loss 2.3531\n",
            "Epoch 1 Batch 109 Loss 2.5026\n",
            "Epoch 1 Batch 110 Loss 2.7007\n",
            "Epoch 1 Batch 111 Loss 2.2039\n",
            "Epoch 1 Batch 112 Loss 2.1144\n",
            "Epoch 1 Batch 113 Loss 2.5852\n",
            "Epoch 1 Batch 114 Loss 2.4180\n",
            "Epoch 1 Batch 115 Loss 2.4074\n",
            "Epoch 1 Batch 116 Loss 2.4599\n",
            "Epoch 1 Batch 117 Loss 2.7609\n",
            "Epoch 1 Batch 118 Loss 1.9316\n",
            "Epoch 1 Batch 119 Loss 2.3279\n",
            "Epoch 1 Batch 120 Loss 2.6905\n",
            "Epoch 1 Batch 121 Loss 2.5458\n",
            "Epoch 1 Batch 122 Loss 2.3752\n",
            "Epoch 1 Batch 123 Loss 2.5754\n",
            "Epoch 1 Batch 124 Loss 2.1812\n",
            "Epoch 1 Batch 125 Loss 2.4453\n",
            "Epoch 1 Batch 126 Loss 2.2529\n",
            "Epoch 1 Batch 127 Loss 2.0401\n",
            "Epoch 1 Batch 128 Loss 2.2361\n",
            "Epoch 1 Batch 129 Loss 2.4043\n",
            "Epoch 1 Batch 130 Loss 2.3173\n",
            "Epoch 1 Batch 131 Loss 2.3157\n",
            "Epoch 1 Batch 132 Loss 2.5285\n",
            "Epoch 1 Batch 133 Loss 2.0813\n",
            "Epoch 1 Batch 134 Loss 2.3253\n",
            "Epoch 1 Batch 135 Loss 2.1516\n",
            "Epoch 1 Batch 136 Loss 2.1411\n",
            "Epoch 1 Batch 137 Loss 2.4127\n",
            "Epoch 1 Batch 138 Loss 1.9401\n",
            "Epoch 1 Batch 139 Loss 2.1217\n",
            "Epoch 1 Batch 140 Loss 2.1460\n",
            "Epoch 1 Batch 141 Loss 2.2015\n",
            "Epoch 1 Batch 142 Loss 2.1682\n",
            "Epoch 1 Batch 143 Loss 2.2463\n",
            "Epoch 1 Batch 144 Loss 2.3723\n",
            "Epoch 1 Batch 145 Loss 2.4458\n",
            "Epoch 1 Batch 146 Loss 2.1478\n",
            "Epoch 1 Batch 147 Loss 2.3660\n",
            "Epoch 1 Batch 148 Loss 1.8838\n",
            "Epoch 1 Batch 149 Loss 2.2085\n",
            "Epoch 1 Batch 150 Loss 2.3146\n",
            "Epoch 1 Batch 151 Loss 2.2124\n",
            "Epoch 1 Batch 152 Loss 2.2378\n",
            "Epoch 1 Batch 153 Loss 2.3229\n",
            "Epoch 1 Batch 154 Loss 2.2608\n",
            "Epoch 1 Batch 155 Loss 2.4840\n",
            "Epoch 1 Batch 156 Loss 2.4031\n",
            "Epoch 1 Batch 157 Loss 2.2888\n",
            "Epoch 1 Batch 158 Loss 2.4682\n",
            "Epoch 1 Batch 159 Loss 2.6103\n",
            "Epoch 1 Batch 160 Loss 2.8466\n",
            "Epoch 1 Batch 161 Loss 2.1145\n",
            "Epoch 1 Batch 162 Loss 2.6656\n",
            "Epoch 1 Batch 163 Loss 2.4218\n",
            "Epoch 1 Batch 164 Loss 2.6337\n",
            "Epoch 1 Batch 165 Loss 2.5052\n",
            "Epoch 1 Batch 166 Loss 2.1292\n",
            "Epoch 1 Batch 167 Loss 2.2255\n",
            "Epoch 1 Batch 168 Loss 2.2881\n",
            "Epoch 1 Batch 169 Loss 2.0879\n",
            "Epoch 1 Batch 170 Loss 1.9190\n",
            "Epoch 1 Batch 171 Loss 2.4914\n",
            "Epoch 1 Batch 172 Loss 2.0837\n",
            "Epoch 1 Batch 173 Loss 2.0390\n",
            "Epoch 1 Batch 174 Loss 2.2754\n",
            "Epoch 1 Batch 175 Loss 2.2996\n",
            "Epoch 1 Batch 176 Loss 2.2837\n",
            "Epoch 1 Batch 177 Loss 1.9022\n",
            "Epoch 1 Batch 178 Loss 2.0871\n",
            "Epoch 1 Batch 179 Loss 2.1436\n",
            "Epoch 1 Batch 180 Loss 2.0143\n",
            "Epoch 1 Batch 181 Loss 2.1713\n",
            "Epoch 1 Batch 182 Loss 2.2165\n",
            "Epoch 1 Batch 183 Loss 2.3387\n",
            "Epoch 1 Batch 184 Loss 2.1662\n",
            "Epoch 1 Batch 185 Loss 2.2814\n",
            "Epoch 1 Batch 186 Loss 2.0377\n",
            "Epoch 1 Batch 187 Loss 2.2697\n",
            "Epoch 1 Batch 188 Loss 2.6953\n",
            "Epoch 1 Batch 189 Loss 2.1880\n",
            "Epoch 1 Batch 190 Loss 2.0239\n",
            "Epoch 1 Batch 191 Loss 1.8006\n",
            "Epoch 1 Batch 192 Loss 1.9938\n",
            "Epoch 1 Batch 193 Loss 2.3388\n",
            "Epoch 1 Batch 194 Loss 1.9910\n",
            "Epoch 1 Batch 195 Loss 2.0461\n",
            "Epoch 1 Batch 196 Loss 2.0073\n",
            "Epoch 1 Batch 197 Loss 2.3356\n",
            "Epoch 1 Batch 198 Loss 2.3462\n",
            "Epoch 1 Batch 199 Loss 2.3984\n",
            "Epoch 1 Batch 200 Loss 2.0027\n",
            "Epoch 1 Batch 201 Loss 2.2353\n",
            "Epoch 1 Batch 202 Loss 1.7267\n",
            "Epoch 1 Batch 203 Loss 2.0715\n",
            "Epoch 1 Batch 204 Loss 2.0071\n",
            "Epoch 1 Batch 205 Loss 2.1671\n",
            "Epoch 1 Batch 206 Loss 2.4886\n",
            "Epoch 1 Batch 207 Loss 2.4801\n",
            "Epoch 1 Batch 208 Loss 2.1820\n",
            "Epoch 1 Batch 209 Loss 2.2960\n",
            "Epoch 1 Batch 210 Loss 1.8101\n",
            "Epoch 1 Batch 211 Loss 1.9661\n",
            "Epoch 1 Batch 212 Loss 2.1793\n",
            "Epoch 1 Batch 213 Loss 2.2284\n",
            "Epoch 1 Batch 214 Loss 2.1724\n",
            "Epoch 1 Batch 215 Loss 2.2300\n",
            "Epoch 1 Batch 216 Loss 2.1577\n",
            "Epoch 1 Batch 217 Loss 2.3229\n",
            "Epoch 1 Batch 218 Loss 1.9551\n",
            "Epoch 1 Batch 219 Loss 1.9855\n",
            "Epoch 1 Batch 220 Loss 2.0638\n",
            "Epoch 1 Batch 221 Loss 2.1062\n",
            "Epoch 1 Batch 222 Loss 2.2721\n",
            "Epoch 1 Batch 223 Loss 2.2197\n",
            "Epoch 1 Batch 224 Loss 2.1417\n",
            "Epoch 1 Batch 225 Loss 2.2684\n",
            "Epoch 1 Batch 226 Loss 1.9662\n",
            "Epoch 1 Batch 227 Loss 2.3669\n",
            "Epoch 1 Batch 228 Loss 2.2100\n",
            "Epoch 1 Batch 229 Loss 2.2224\n",
            "Epoch 1 Batch 230 Loss 2.0031\n",
            "Epoch 1 Batch 231 Loss 1.9869\n",
            "Epoch 1 Batch 232 Loss 2.1043\n",
            "Epoch 1 Batch 233 Loss 2.4216\n",
            "Epoch 1 Batch 234 Loss 2.0468\n",
            "Epoch 1 Batch 235 Loss 2.0164\n",
            "Epoch 1 Batch 236 Loss 2.1063\n",
            "Epoch 1 Batch 237 Loss 1.9351\n",
            "Epoch 1 Batch 238 Loss 2.1953\n",
            "Epoch 1 Batch 239 Loss 2.0033\n",
            "Epoch 1 Batch 240 Loss 1.9483\n",
            "Epoch 1 Batch 241 Loss 2.2602\n",
            "Epoch 1 Batch 242 Loss 2.0263\n",
            "Epoch 1 Batch 243 Loss 2.2133\n",
            "Epoch 1 Batch 244 Loss 2.2557\n",
            "Epoch 1 Batch 245 Loss 2.1869\n",
            "Epoch 1 Batch 246 Loss 1.7766\n",
            "Epoch 1 Batch 247 Loss 2.2389\n",
            "Epoch 1 Batch 248 Loss 2.1008\n",
            "Epoch 1 Batch 249 Loss 2.0631\n",
            "Epoch 1 Batch 250 Loss 2.1299\n",
            "Epoch 1 Batch 251 Loss 2.2591\n",
            "Epoch 1 Batch 252 Loss 2.2799\n",
            "Epoch 1 Batch 253 Loss 1.9090\n",
            "Epoch 1 Batch 254 Loss 1.9381\n",
            "Epoch 1 Batch 255 Loss 1.9446\n",
            "Epoch 1 Batch 256 Loss 2.0165\n",
            "Epoch 1 Batch 257 Loss 2.0239\n",
            "Epoch 1 Batch 258 Loss 1.8168\n",
            "Epoch 1 Batch 259 Loss 2.0558\n",
            "Epoch 1 Batch 260 Loss 1.8071\n",
            "Epoch 1 Batch 261 Loss 1.9393\n",
            "Epoch 1 Batch 262 Loss 2.0711\n",
            "Epoch 1 Batch 263 Loss 2.2342\n",
            "Epoch 1 Batch 264 Loss 2.2312\n",
            "Epoch 1 Batch 265 Loss 2.0982\n",
            "Epoch 1 Batch 266 Loss 2.2127\n",
            "Epoch 1 Batch 267 Loss 1.9989\n",
            "Epoch 1 Batch 268 Loss 2.2353\n",
            "Epoch 1 Batch 269 Loss 2.1014\n",
            "Epoch 1 Batch 270 Loss 1.9018\n",
            "Epoch 1 Batch 271 Loss 1.9616\n",
            "Epoch 1 Batch 272 Loss 2.0165\n",
            "Epoch 1 Batch 273 Loss 2.3128\n",
            "Epoch 1 Batch 274 Loss 2.2464\n",
            "Epoch 1 Batch 275 Loss 2.2843\n",
            "Epoch 1 Batch 276 Loss 2.2714\n",
            "Epoch 1 Batch 277 Loss 2.0963\n",
            "Epoch 1 Batch 278 Loss 1.9216\n",
            "Epoch 1 Batch 279 Loss 1.9782\n",
            "Epoch 1 Batch 280 Loss 2.1342\n",
            "Epoch 1 Batch 281 Loss 2.0591\n",
            "Epoch 1 Batch 282 Loss 1.7870\n",
            "Epoch 1 Batch 283 Loss 2.4426\n",
            "Epoch 1 Batch 284 Loss 1.9625\n",
            "Epoch 1 Batch 285 Loss 2.0883\n",
            "Epoch 1 Batch 286 Loss 2.1255\n",
            "Epoch 1 Batch 287 Loss 2.1420\n",
            "Epoch 1 Batch 288 Loss 2.1881\n",
            "Epoch 1 Batch 289 Loss 1.9469\n",
            "Epoch 1 Batch 290 Loss 1.9865\n",
            "Epoch 1 Batch 291 Loss 2.2256\n",
            "Epoch 1 Batch 292 Loss 1.9927\n",
            "Epoch 1 Batch 293 Loss 2.2228\n",
            "Epoch 1 Batch 294 Loss 2.0305\n",
            "Epoch 1 Batch 295 Loss 1.8927\n",
            "Epoch 1 Batch 296 Loss 2.3857\n",
            "Epoch 1 Batch 297 Loss 2.0381\n",
            "Epoch 1 Batch 298 Loss 2.0342\n",
            "Epoch 1 Batch 299 Loss 2.1239\n",
            "Epoch 1 Batch 300 Loss 1.9934\n",
            "Epoch 1 Batch 301 Loss 1.8659\n",
            "Epoch 1 Batch 302 Loss 1.8948\n",
            "Epoch 1 Batch 303 Loss 1.9161\n",
            "Epoch 1 Batch 304 Loss 1.8304\n",
            "Epoch 1 Batch 305 Loss 2.0198\n",
            "Epoch 1 Batch 306 Loss 2.1864\n",
            "Epoch 1 Batch 307 Loss 1.9307\n",
            "Epoch 1 Batch 308 Loss 2.2176\n",
            "Epoch 1 Batch 309 Loss 1.7366\n",
            "Epoch 1 Batch 310 Loss 2.4407\n",
            "Epoch 1 Batch 311 Loss 1.9023\n",
            "Epoch 1 Batch 312 Loss 1.9977\n",
            "Epoch 1 Batch 313 Loss 2.1451\n",
            "Epoch 1 Batch 314 Loss 1.8802\n",
            "Epoch 1 Batch 315 Loss 2.0040\n",
            "Epoch 1 Batch 316 Loss 2.0881\n",
            "Epoch 1 Batch 317 Loss 1.8933\n",
            "Epoch 1 Batch 318 Loss 2.0069\n",
            "Epoch 1 Batch 319 Loss 1.8795\n",
            "Epoch 1 Batch 320 Loss 1.9812\n",
            "Epoch 1 Batch 321 Loss 1.7934\n",
            "Epoch 1 Batch 322 Loss 2.0281\n",
            "Epoch 1 Batch 323 Loss 2.0687\n",
            "Epoch 1 Batch 324 Loss 1.9955\n",
            "Epoch 1 Batch 325 Loss 1.8037\n",
            "Epoch 1 Batch 326 Loss 2.0550\n",
            "Epoch 1 Batch 327 Loss 2.0427\n",
            "Epoch 1 Batch 328 Loss 2.0534\n",
            "Epoch 1 Batch 329 Loss 2.1070\n",
            "Epoch 1 Batch 330 Loss 2.1060\n",
            "Epoch 1 Batch 331 Loss 2.0609\n",
            "Epoch 1 Batch 332 Loss 2.1266\n",
            "Epoch 1 Batch 333 Loss 2.2503\n",
            "Epoch 1 Batch 334 Loss 2.1186\n",
            "Epoch 1 Batch 335 Loss 1.9734\n",
            "Epoch 1 Batch 336 Loss 2.0321\n",
            "Epoch 1 Batch 337 Loss 2.0154\n",
            "Epoch 1 Batch 338 Loss 1.9098\n",
            "Epoch 1 Batch 339 Loss 2.1249\n",
            "Epoch 1 Batch 340 Loss 2.1541\n",
            "Epoch 1 Batch 341 Loss 1.9276\n",
            "Epoch 1 Batch 342 Loss 1.9562\n",
            "Epoch 1 Batch 343 Loss 1.8971\n",
            "Epoch 1 Batch 344 Loss 2.1176\n",
            "Epoch 1 Batch 345 Loss 2.0506\n",
            "Epoch 1 Batch 346 Loss 2.1050\n",
            "Epoch 1 Batch 347 Loss 2.1727\n",
            "Epoch 1 Batch 348 Loss 1.7267\n",
            "Epoch 1 Batch 349 Loss 1.8497\n",
            "Epoch 1 Batch 350 Loss 2.0018\n",
            "Epoch 1 Batch 351 Loss 1.7837\n",
            "Epoch 1 Batch 352 Loss 1.9170\n",
            "Epoch 1 Batch 353 Loss 2.0036\n",
            "Epoch 1 Batch 354 Loss 2.1639\n",
            "Epoch 1 Batch 355 Loss 1.8661\n",
            "Epoch 1 Batch 356 Loss 1.7604\n",
            "Epoch 1 Batch 357 Loss 2.0211\n",
            "Epoch 1 Batch 358 Loss 1.9474\n",
            "Epoch 1 Batch 359 Loss 1.9102\n",
            "Epoch 1 Batch 360 Loss 1.8582\n",
            "Epoch 1 Batch 361 Loss 1.5976\n",
            "Epoch 1 Batch 362 Loss 1.9630\n",
            "Epoch 1 Batch 363 Loss 2.1712\n",
            "Epoch 1 Batch 364 Loss 1.7485\n",
            "Epoch 1 Batch 365 Loss 2.1077\n",
            "Epoch 1 Batch 366 Loss 1.9517\n",
            "Epoch 1 Batch 367 Loss 2.2149\n",
            "Epoch 1 Batch 368 Loss 1.8333\n",
            "Epoch 1 Batch 369 Loss 1.8958\n",
            "Epoch 1 Batch 370 Loss 1.9988\n",
            "Epoch 1 Batch 371 Loss 1.9481\n",
            "Epoch 1 Batch 372 Loss 1.9559\n",
            "Epoch 1 Batch 373 Loss 1.8672\n",
            "Epoch 1 Batch 374 Loss 1.8250\n",
            "Epoch 1 Batch 375 Loss 1.9021\n",
            "Epoch 1 Batch 376 Loss 2.0706\n",
            "Epoch 1 Batch 377 Loss 1.7258\n",
            "Epoch 1 Batch 378 Loss 1.7675\n",
            "Epoch 1 Batch 379 Loss 1.9892\n",
            "Epoch 1 Batch 380 Loss 1.7232\n",
            "Epoch 1 Batch 381 Loss 1.7570\n",
            "Epoch 1 Batch 382 Loss 2.1902\n",
            "Epoch 1 Batch 383 Loss 2.0877\n",
            "Epoch 1 Batch 384 Loss 2.2173\n",
            "Epoch 1 Batch 385 Loss 2.0252\n",
            "Epoch 1 Batch 386 Loss 1.8562\n",
            "Epoch 1 Batch 387 Loss 1.8752\n",
            "Epoch 1 Batch 388 Loss 1.8160\n",
            "Epoch 1 Batch 389 Loss 1.9511\n",
            "Epoch 1 Batch 390 Loss 2.1799\n",
            "Epoch 1 Batch 391 Loss 1.6139\n",
            "Epoch 1 Batch 392 Loss 1.9884\n",
            "Epoch 1 Batch 393 Loss 2.1417\n",
            "Epoch 1 Batch 394 Loss 2.0130\n",
            "Epoch 1 Batch 395 Loss 1.8808\n",
            "Epoch 1 Batch 396 Loss 1.9724\n",
            "Epoch 1 Batch 397 Loss 2.2103\n",
            "Epoch 1 Batch 398 Loss 2.0513\n",
            "Epoch 1 Batch 399 Loss 1.7296\n",
            "Epoch 1 Batch 400 Loss 1.9882\n",
            "Epoch 1 Batch 401 Loss 1.7829\n",
            "Epoch 1 Batch 402 Loss 1.6988\n",
            "Epoch 1 Batch 403 Loss 1.7809\n",
            "Epoch 1 Batch 404 Loss 1.7660\n",
            "Epoch 1 Batch 405 Loss 1.8092\n",
            "Epoch 1 Batch 406 Loss 1.9012\n",
            "Epoch 1 Batch 407 Loss 2.0709\n",
            "Epoch 1 Batch 408 Loss 1.7211\n",
            "Epoch 1 Batch 409 Loss 1.8372\n",
            "Epoch 1 Batch 410 Loss 2.0107\n",
            "Epoch 1 Batch 411 Loss 1.9160\n",
            "Epoch 1 Batch 412 Loss 1.6795\n",
            "Epoch 1 Batch 413 Loss 1.7557\n",
            "Epoch 1 Batch 414 Loss 1.9003\n",
            "Epoch 1 Batch 415 Loss 1.8061\n",
            "Epoch 1 Batch 416 Loss 1.8583\n",
            "Epoch 1 Batch 417 Loss 1.8682\n",
            "Epoch 1 Batch 418 Loss 2.0331\n",
            "Epoch 1 Batch 419 Loss 1.7868\n",
            "Epoch 1 Batch 420 Loss 1.9063\n",
            "Epoch 1 Batch 421 Loss 1.6077\n",
            "Epoch 1 Batch 422 Loss 1.8138\n",
            "Epoch 1 Batch 423 Loss 1.9358\n",
            "Epoch 1 Batch 424 Loss 1.9365\n",
            "Epoch 1 Batch 425 Loss 1.9095\n",
            "Epoch 1 Batch 426 Loss 1.8979\n",
            "Epoch 1 Batch 427 Loss 1.9398\n",
            "Epoch 1 Batch 428 Loss 1.7448\n",
            "Epoch 1 Batch 429 Loss 1.8384\n",
            "Epoch 1 Batch 430 Loss 2.0537\n",
            "Epoch 1 Batch 431 Loss 1.8554\n",
            "Epoch 1 Batch 432 Loss 2.0743\n",
            "Epoch 1 Batch 433 Loss 2.1490\n",
            "Epoch 1 Batch 434 Loss 1.7963\n",
            "Epoch 1 Batch 435 Loss 2.0400\n",
            "Epoch 1 Batch 436 Loss 1.9468\n",
            "Epoch 1 Batch 437 Loss 1.8105\n",
            "Epoch 1 Batch 438 Loss 1.8790\n",
            "Epoch 1 Batch 439 Loss 2.1450\n",
            "Epoch 1 Batch 440 Loss 1.7528\n",
            "Epoch 1 Batch 441 Loss 1.8897\n",
            "Epoch 1 Batch 442 Loss 1.9524\n",
            "Epoch 1 Batch 443 Loss 2.0037\n",
            "Epoch 1 Batch 444 Loss 2.0877\n",
            "Epoch 1 Batch 445 Loss 1.9703\n",
            "Epoch 1 Batch 446 Loss 1.5532\n",
            "Epoch 1 Batch 447 Loss 2.0082\n",
            "Epoch 1 Batch 448 Loss 2.0037\n",
            "Epoch 1 Batch 449 Loss 1.8236\n",
            "Epoch 1 Batch 450 Loss 2.0690\n",
            "Epoch 1 Batch 451 Loss 2.2307\n",
            "Epoch 1 Batch 452 Loss 1.8680\n",
            "Epoch 1 Batch 453 Loss 2.0921\n",
            "Epoch 1 Batch 454 Loss 2.0565\n",
            "Epoch 1 Batch 455 Loss 1.7709\n",
            "Epoch 1 Batch 456 Loss 1.9120\n",
            "Epoch 1 Batch 457 Loss 1.8127\n",
            "Epoch 1 Batch 458 Loss 1.8341\n",
            "Epoch 1 Batch 459 Loss 1.8076\n",
            "Epoch 1 Batch 460 Loss 2.1289\n",
            "Epoch 1 Batch 461 Loss 1.9094\n",
            "Epoch 1 Batch 462 Loss 2.0568\n",
            "Epoch 1 Batch 463 Loss 1.9426\n",
            "Epoch 1 Batch 464 Loss 2.0886\n",
            "Epoch 1 Batch 465 Loss 1.8528\n",
            "Epoch 1 Batch 466 Loss 1.6610\n",
            "Epoch 1 Batch 467 Loss 1.8229\n",
            "Epoch 1 Batch 468 Loss 1.9901\n",
            "Epoch 1 Batch 469 Loss 1.6771\n",
            "Epoch 1 Batch 470 Loss 1.8928\n",
            "Epoch 1 Batch 471 Loss 1.7782\n",
            "Epoch 1 Batch 472 Loss 1.8360\n",
            "Epoch 1 Batch 473 Loss 1.9955\n",
            "Epoch 1 Batch 474 Loss 1.8940\n",
            "Epoch 1 Batch 475 Loss 2.0065\n",
            "Epoch 1 Batch 476 Loss 1.7077\n",
            "Epoch 1 Batch 477 Loss 1.8424\n",
            "Epoch 1 Batch 478 Loss 1.8166\n",
            "Epoch 1 Batch 479 Loss 1.9208\n",
            "Epoch 1 Batch 480 Loss 1.7475\n",
            "Epoch 1 Batch 481 Loss 1.6752\n",
            "Epoch 1 Batch 482 Loss 1.7433\n",
            "Epoch 1 Batch 483 Loss 1.9711\n",
            "Epoch 1 Batch 484 Loss 1.8397\n",
            "Epoch 1 Batch 485 Loss 1.7942\n",
            "Epoch 1 Batch 486 Loss 1.5897\n",
            "Epoch 1 Batch 487 Loss 1.8394\n",
            "Epoch 1 Batch 488 Loss 1.7081\n",
            "Epoch 1 Batch 489 Loss 1.9566\n",
            "Epoch 1 Batch 490 Loss 1.8934\n",
            "Epoch 1 Batch 491 Loss 1.8862\n",
            "Epoch 1 Batch 492 Loss 2.1394\n",
            "Epoch 1 Batch 493 Loss 2.0393\n",
            "Epoch 1 Batch 494 Loss 1.8802\n",
            "Epoch 1 Batch 495 Loss 2.1017\n",
            "Epoch 1 Batch 496 Loss 2.1204\n",
            "Epoch 1 Batch 497 Loss 1.7249\n",
            "Epoch 1 Batch 498 Loss 1.7464\n",
            "Epoch 1 Batch 499 Loss 1.7898\n",
            "Epoch 1 Batch 500 Loss 1.8761\n",
            "Epoch 1 Batch 501 Loss 1.6713\n",
            "Epoch 1 Batch 502 Loss 1.7167\n",
            "Epoch 1 Batch 503 Loss 2.0904\n",
            "Epoch 1 Batch 504 Loss 1.7120\n",
            "Epoch 1 Batch 505 Loss 2.0176\n",
            "Epoch 1 Batch 506 Loss 1.5330\n",
            "Epoch 1 Batch 507 Loss 2.0280\n",
            "Epoch 1 Batch 508 Loss 1.8252\n",
            "Epoch 1 Batch 509 Loss 1.8047\n",
            "Epoch 1 Batch 510 Loss 1.6882\n",
            "Epoch 1 Batch 511 Loss 1.8115\n",
            "Epoch 1 Batch 512 Loss 1.9618\n",
            "Epoch 1 Batch 513 Loss 1.8422\n",
            "Epoch 1 Batch 514 Loss 1.9661\n",
            "Epoch 1 Batch 515 Loss 1.7596\n",
            "Epoch 1 Batch 516 Loss 1.6991\n",
            "Epoch 1 Batch 517 Loss 1.6825\n",
            "Epoch 1 Batch 518 Loss 1.9977\n",
            "Epoch 1 Batch 519 Loss 1.6054\n",
            "Epoch 1 Batch 520 Loss 1.7749\n",
            "Epoch 1 Batch 521 Loss 2.0056\n",
            "Epoch 1 Batch 522 Loss 2.0919\n",
            "Epoch 1 Batch 523 Loss 1.8551\n",
            "Epoch 1 Batch 524 Loss 1.7793\n",
            "Epoch 1 Batch 525 Loss 2.1806\n",
            "Epoch 1 Batch 526 Loss 1.5698\n",
            "Epoch 1 Batch 527 Loss 2.0368\n",
            "Epoch 1 Batch 528 Loss 1.8563\n",
            "Epoch 1 Batch 529 Loss 2.2814\n",
            "Epoch 1 Batch 530 Loss 2.0057\n",
            "Epoch 1 Batch 531 Loss 2.0379\n",
            "Epoch 1 Batch 532 Loss 1.6299\n",
            "Epoch 1 Batch 533 Loss 1.7642\n",
            "Epoch 1 Batch 534 Loss 1.9848\n",
            "Epoch 1 Batch 535 Loss 1.6969\n",
            "Epoch 1 Batch 536 Loss 1.8852\n",
            "Epoch 1 Batch 537 Loss 1.8896\n",
            "Epoch 1 Batch 538 Loss 1.9456\n",
            "Epoch 1 Batch 539 Loss 1.8660\n",
            "Epoch 1 Batch 540 Loss 1.9093\n",
            "Epoch 1 Batch 541 Loss 1.5381\n",
            "Epoch 1 Batch 542 Loss 1.8469\n",
            "Epoch 1 Batch 543 Loss 1.7292\n",
            "Epoch 1 Batch 544 Loss 1.6288\n",
            "Epoch 1 Batch 545 Loss 1.6703\n",
            "Epoch 1 Batch 546 Loss 1.7865\n",
            "Epoch 1 Batch 547 Loss 1.8176\n",
            "Epoch 1 Batch 548 Loss 1.7965\n",
            "Epoch 1 Batch 549 Loss 1.7964\n",
            "Epoch 1 Batch 550 Loss 1.9277\n",
            "Epoch 1 Batch 551 Loss 1.7914\n",
            "Epoch 1 Batch 552 Loss 1.7847\n",
            "Epoch 1 Batch 553 Loss 1.9426\n",
            "Epoch 1 Batch 554 Loss 1.6295\n",
            "Epoch 1 Batch 555 Loss 2.0264\n",
            "Epoch 1 Batch 556 Loss 1.7677\n",
            "Epoch 1 Batch 557 Loss 1.8113\n",
            "Epoch 1 Batch 558 Loss 1.8186\n",
            "Epoch 1 Batch 559 Loss 1.6809\n",
            "Epoch 1 Batch 560 Loss 1.5320\n",
            "Epoch 1 Batch 561 Loss 1.7594\n",
            "Epoch 1 Batch 562 Loss 1.8866\n",
            "Epoch 1 Batch 563 Loss 1.7539\n",
            "Epoch 1 Batch 564 Loss 1.8369\n",
            "Epoch 1 Batch 565 Loss 1.6419\n",
            "Epoch 1 Batch 566 Loss 1.3868\n",
            "Epoch 1 Batch 567 Loss 1.6365\n",
            "Epoch 1 Batch 568 Loss 1.7834\n",
            "Epoch 1 Batch 569 Loss 1.9257\n",
            "Epoch 1 Batch 570 Loss 2.1201\n",
            "Epoch 1 Batch 571 Loss 1.6733\n",
            "Epoch 1 Batch 572 Loss 1.8931\n",
            "Epoch 1 Batch 573 Loss 1.6685\n",
            "Epoch 1 Batch 574 Loss 1.9199\n",
            "Epoch 1 Batch 575 Loss 1.6935\n",
            "Epoch 1 Batch 576 Loss 1.7964\n",
            "Epoch 1 Batch 577 Loss 1.8927\n",
            "Epoch 1 Batch 578 Loss 1.8323\n",
            "Epoch 1 Batch 579 Loss 1.8419\n",
            "Epoch 1 Batch 580 Loss 1.8217\n",
            "Epoch 1 Batch 581 Loss 1.8071\n",
            "Epoch 1 Batch 582 Loss 1.9718\n",
            "Epoch 1 Batch 583 Loss 1.6974\n",
            "Epoch 1 Batch 584 Loss 1.8681\n",
            "Epoch 1 Batch 585 Loss 1.9736\n",
            "Epoch 1 Batch 586 Loss 1.8672\n",
            "Epoch 1 Batch 587 Loss 1.7686\n",
            "Epoch 1 Batch 588 Loss 1.8692\n",
            "Epoch 1 Batch 589 Loss 1.5086\n",
            "Epoch 1 Batch 590 Loss 1.6681\n",
            "Epoch 1 Batch 591 Loss 1.7422\n",
            "Epoch 1 Batch 592 Loss 1.9186\n",
            "Epoch 1 Batch 593 Loss 1.6568\n",
            "Epoch 1 Batch 594 Loss 1.8398\n",
            "Epoch 1 Batch 595 Loss 1.4418\n",
            "Epoch 1 Batch 596 Loss 1.8242\n",
            "Epoch 1 Batch 597 Loss 1.9992\n",
            "Epoch 1 Batch 598 Loss 1.9507\n",
            "Epoch 1 Batch 599 Loss 1.7190\n",
            "Epoch 1 Batch 600 Loss 1.8509\n",
            "Epoch 1 Batch 601 Loss 1.8502\n",
            "Epoch 1 Batch 602 Loss 1.6989\n",
            "Epoch 1 Batch 603 Loss 1.7527\n",
            "Epoch 1 Batch 604 Loss 1.6661\n",
            "Epoch 1 Batch 605 Loss 1.6723\n",
            "Epoch 1 Batch 606 Loss 1.7101\n",
            "Epoch 1 Batch 607 Loss 1.9104\n",
            "Epoch 1 Batch 608 Loss 1.9344\n",
            "Epoch 1 Batch 609 Loss 1.7065\n",
            "Epoch 1 Batch 610 Loss 1.5454\n",
            "Epoch 1 Batch 611 Loss 1.5318\n",
            "Epoch 1 Batch 612 Loss 1.8849\n",
            "Epoch 1 Batch 613 Loss 1.7792\n",
            "Epoch 1 Batch 614 Loss 1.9249\n",
            "Epoch 1 Batch 615 Loss 1.8333\n",
            "Epoch 1 Batch 616 Loss 1.6197\n",
            "Epoch 1 Batch 617 Loss 1.9844\n",
            "Epoch 1 Batch 618 Loss 1.5555\n",
            "Epoch 1 Batch 619 Loss 1.6975\n",
            "Epoch 1 Batch 620 Loss 1.9154\n",
            "Epoch 1 Batch 621 Loss 1.8670\n",
            "Epoch 1 Batch 622 Loss 1.7108\n",
            "Epoch 1 Batch 623 Loss 1.7494\n",
            "Epoch 1 Batch 624 Loss 1.7313\n",
            "Epoch 1 Batch 625 Loss 1.7457\n",
            "Epoch 1 Batch 626 Loss 1.8502\n",
            "Epoch 1 Batch 627 Loss 1.9398\n",
            "Epoch 1 Batch 628 Loss 1.7954\n",
            "Epoch 1 Batch 629 Loss 1.8867\n",
            "Epoch 1 Batch 630 Loss 1.5205\n",
            "Epoch 1 Batch 631 Loss 1.6657\n",
            "Epoch 1 Batch 632 Loss 1.9641\n",
            "Epoch 1 Batch 633 Loss 1.9717\n",
            "Epoch 1 Batch 634 Loss 1.6724\n",
            "Epoch 1 Batch 635 Loss 1.9714\n",
            "Epoch 1 Batch 636 Loss 1.9997\n",
            "Epoch 1 Batch 637 Loss 1.7276\n",
            "Epoch 1 Batch 638 Loss 1.7116\n",
            "Epoch 1 Batch 639 Loss 1.5513\n",
            "Epoch 1 Batch 640 Loss 2.0036\n",
            "Epoch 1 Batch 641 Loss 1.4966\n",
            "Epoch 1 Batch 642 Loss 1.5944\n",
            "Epoch 1 Batch 643 Loss 1.4000\n",
            "Epoch 1 Batch 644 Loss 1.6993\n",
            "Epoch 1 Batch 645 Loss 1.3885\n",
            "Epoch 1 Batch 646 Loss 1.5331\n",
            "Epoch 1 Batch 647 Loss 1.6426\n",
            "Epoch 1 Batch 648 Loss 1.7592\n",
            "Epoch 1 Batch 649 Loss 1.8891\n",
            "Epoch 1 Batch 650 Loss 1.8868\n",
            "Epoch 1 Batch 651 Loss 1.8541\n",
            "Epoch 1 Batch 652 Loss 1.5035\n",
            "Epoch 1 Batch 653 Loss 1.5706\n",
            "Epoch 1 Batch 654 Loss 1.6188\n",
            "Epoch 1 Batch 655 Loss 1.8319\n",
            "Epoch 1 Batch 656 Loss 1.9851\n",
            "Epoch 1 Batch 657 Loss 1.6996\n",
            "Epoch 1 Batch 658 Loss 1.5630\n",
            "Epoch 1 Batch 659 Loss 1.6622\n",
            "Epoch 1 Batch 660 Loss 1.7412\n",
            "Epoch 1 Batch 661 Loss 1.4600\n",
            "Epoch 1 Batch 662 Loss 1.9760\n",
            "Epoch 1 Batch 663 Loss 1.6688\n",
            "Epoch 1 Batch 664 Loss 1.6928\n",
            "Epoch 1 Batch 665 Loss 1.8325\n",
            "Epoch 1 Batch 666 Loss 1.7908\n",
            "Epoch 1 Batch 667 Loss 1.8648\n",
            "Epoch 1 Batch 668 Loss 1.6843\n",
            "Epoch 1 Batch 669 Loss 1.6663\n",
            "Epoch 1 Batch 670 Loss 1.8738\n",
            "Epoch 1 Batch 671 Loss 1.8716\n",
            "Epoch 1 Batch 672 Loss 1.7085\n",
            "Epoch 1 Batch 673 Loss 1.6898\n",
            "Epoch 1 Batch 674 Loss 1.4985\n",
            "Epoch 1 Batch 675 Loss 1.9428\n",
            "Epoch 1 Batch 676 Loss 1.9895\n",
            "Epoch 1 Batch 677 Loss 1.7002\n",
            "Epoch 1 Batch 678 Loss 1.8005\n",
            "Epoch 1 Batch 679 Loss 1.6851\n",
            "Epoch 1 Batch 680 Loss 1.8868\n",
            "Epoch 1 Batch 681 Loss 1.7803\n",
            "Epoch 1 Batch 682 Loss 1.6312\n",
            "Epoch 1 Batch 683 Loss 1.7726\n",
            "Epoch 1 Batch 684 Loss 1.6082\n",
            "Epoch 1 Batch 685 Loss 1.7951\n",
            "Epoch 1 Batch 686 Loss 1.7795\n",
            "Epoch 1 Batch 687 Loss 1.7550\n",
            "Epoch 1 Batch 688 Loss 1.5376\n",
            "Epoch 1 Batch 689 Loss 1.8293\n",
            "Epoch 1 Batch 690 Loss 1.8819\n",
            "Epoch 1 Batch 691 Loss 1.8133\n",
            "Epoch 1 Batch 692 Loss 1.6964\n",
            "Epoch 1 Batch 693 Loss 1.6746\n",
            "Epoch 1 Batch 694 Loss 1.6134\n",
            "Epoch 1 Batch 695 Loss 1.5815\n",
            "Epoch 1 Batch 696 Loss 1.7015\n",
            "Epoch 1 Batch 697 Loss 1.7265\n",
            "Epoch 1 Batch 698 Loss 1.6889\n",
            "Epoch 1 Batch 699 Loss 1.5456\n",
            "Epoch 1 Batch 700 Loss 1.4575\n",
            "Epoch 1 Batch 701 Loss 1.8422\n",
            "Epoch 1 Batch 702 Loss 1.7470\n",
            "Epoch 1 Batch 703 Loss 1.7002\n",
            "Epoch 1 Batch 704 Loss 2.0558\n",
            "Epoch 1 Batch 705 Loss 1.9977\n",
            "Epoch 1 Batch 706 Loss 1.5187\n",
            "Epoch 1 Batch 707 Loss 1.4485\n",
            "Epoch 1 Batch 708 Loss 1.7573\n",
            "Epoch 1 Batch 709 Loss 1.7393\n",
            "Epoch 1 Batch 710 Loss 1.8573\n",
            "Epoch 1 Batch 711 Loss 1.5276\n",
            "Epoch 1 Batch 712 Loss 1.6370\n",
            "Epoch 1 Batch 713 Loss 1.6179\n",
            "Epoch 1 Batch 714 Loss 1.5916\n",
            "Epoch 1 Batch 715 Loss 1.7466\n",
            "Epoch 1 Batch 716 Loss 1.6772\n",
            "Epoch 1 Batch 717 Loss 1.7523\n",
            "Epoch 1 Batch 718 Loss 1.4646\n",
            "Epoch 1 Batch 719 Loss 1.9236\n",
            "Epoch 1 Batch 720 Loss 1.7203\n",
            "Epoch 1 Batch 721 Loss 1.6908\n",
            "Epoch 1 Batch 722 Loss 1.8078\n",
            "Epoch 1 Batch 723 Loss 1.8307\n",
            "Epoch 1 Batch 724 Loss 1.6942\n",
            "Epoch 1 Batch 725 Loss 1.7565\n",
            "Epoch 1 Batch 726 Loss 1.4827\n",
            "Epoch 1 Batch 727 Loss 1.6285\n",
            "Epoch 1 Batch 728 Loss 2.0400\n",
            "Epoch 1 Batch 729 Loss 1.6480\n",
            "Epoch 1 Batch 730 Loss 1.8096\n",
            "Epoch 1 Batch 731 Loss 1.6243\n",
            "Epoch 1 Batch 732 Loss 1.7883\n",
            "Epoch 1 Batch 733 Loss 1.4718\n",
            "Epoch 1 Batch 734 Loss 2.0869\n",
            "Epoch 1 Batch 735 Loss 1.6133\n",
            "Epoch 1 Batch 736 Loss 1.6687\n",
            "Epoch 1 Batch 737 Loss 1.8366\n",
            "Epoch 1 Batch 738 Loss 1.7914\n",
            "Epoch 1 Batch 739 Loss 1.6474\n",
            "Epoch 1 Batch 740 Loss 2.3058\n",
            "Epoch 1 Batch 741 Loss 1.7227\n",
            "Epoch 1 Batch 742 Loss 1.8842\n",
            "Epoch 1 Batch 743 Loss 1.8292\n",
            "Epoch 1 Batch 744 Loss 1.8107\n",
            "Epoch 1 Batch 745 Loss 1.7352\n",
            "Epoch 1 Batch 746 Loss 1.6455\n",
            "Epoch 1 Batch 747 Loss 1.5831\n",
            "Epoch 1 Batch 748 Loss 1.7640\n",
            "Epoch 1 Batch 749 Loss 1.9151\n",
            "Epoch 1 Batch 750 Loss 1.5726\n",
            "Epoch 1 Batch 751 Loss 1.7512\n",
            "Epoch 1 Batch 752 Loss 1.8603\n",
            "Epoch 1 Batch 753 Loss 1.6274\n",
            "Epoch 1 Batch 754 Loss 1.6597\n",
            "Epoch 1 Batch 755 Loss 1.5426\n",
            "Epoch 1 Batch 756 Loss 1.4911\n",
            "Epoch 1 Batch 757 Loss 1.5747\n",
            "Epoch 1 Batch 758 Loss 1.4720\n",
            "Epoch 1 Batch 759 Loss 1.5737\n",
            "Epoch 1 Batch 760 Loss 1.6907\n",
            "Epoch 1 Batch 761 Loss 1.7749\n",
            "Epoch 1 Batch 762 Loss 1.9107\n",
            "Epoch 1 Batch 763 Loss 1.8605\n",
            "Epoch 1 Batch 764 Loss 2.0410\n",
            "Epoch 1 Batch 765 Loss 1.9460\n",
            "Epoch 1 Batch 766 Loss 1.6342\n",
            "Epoch 1 Batch 767 Loss 1.6829\n",
            "Epoch 1 Batch 768 Loss 1.7359\n",
            "Epoch 1 Batch 769 Loss 1.7826\n",
            "Epoch 1 Batch 770 Loss 1.8215\n",
            "Epoch 1 Batch 771 Loss 1.7446\n",
            "Epoch 1 Batch 772 Loss 1.4950\n",
            "Epoch 1 Batch 773 Loss 1.6459\n",
            "Epoch 1 Batch 774 Loss 1.4126\n",
            "Epoch 1 Batch 775 Loss 1.7214\n",
            "Epoch 1 Batch 776 Loss 2.1243\n",
            "Epoch 1 Batch 777 Loss 1.9711\n",
            "Epoch 1 Batch 778 Loss 2.0127\n",
            "Epoch 1 Batch 779 Loss 1.7230\n",
            "Epoch 1 Batch 780 Loss 1.7734\n",
            "Epoch 1 Batch 781 Loss 1.9373\n",
            "Epoch 1 Batch 782 Loss 1.8109\n",
            "Epoch 1 Batch 783 Loss 1.8546\n",
            "Epoch 1 Batch 784 Loss 1.6843\n",
            "Epoch 1 Batch 785 Loss 1.6133\n",
            "Epoch 1 Batch 786 Loss 1.7669\n",
            "Epoch 1 Batch 787 Loss 1.6821\n",
            "Epoch 1 Batch 788 Loss 1.8450\n",
            "Epoch 1 Batch 789 Loss 1.6871\n",
            "Epoch 1 Batch 790 Loss 1.4434\n",
            "Epoch 1 Batch 791 Loss 1.7354\n",
            "Epoch 1 Batch 792 Loss 1.6977\n",
            "Epoch 1 Batch 793 Loss 1.4931\n",
            "Epoch 1 Batch 794 Loss 1.7664\n",
            "Epoch 1 Batch 795 Loss 1.6087\n",
            "Epoch 1 Batch 796 Loss 1.6738\n",
            "Epoch 1 Batch 797 Loss 1.9995\n",
            "Epoch 1 Batch 798 Loss 1.8132\n",
            "Epoch 1 Batch 799 Loss 1.7823\n",
            "Epoch 1 Batch 800 Loss 1.8860\n",
            "Epoch 1 Batch 801 Loss 1.8196\n",
            "Epoch 1 Batch 802 Loss 1.7233\n",
            "Epoch 1 Batch 803 Loss 1.7514\n",
            "Epoch 1 Batch 804 Loss 1.7064\n",
            "Epoch 1 Batch 805 Loss 1.8128\n",
            "Epoch 1 Batch 806 Loss 1.8198\n",
            "Epoch 1 Batch 807 Loss 1.6464\n",
            "Epoch 1 Batch 808 Loss 1.9930\n",
            "Epoch 1 Batch 809 Loss 1.7594\n",
            "Epoch 1 Batch 810 Loss 1.6061\n",
            "Epoch 1 Batch 811 Loss 1.8736\n",
            "Epoch 1 Batch 812 Loss 1.6436\n",
            "Epoch 1 Batch 813 Loss 1.7454\n",
            "Epoch 1 Batch 814 Loss 1.9031\n",
            "Epoch 1 Batch 815 Loss 1.7034\n",
            "Epoch 1 Batch 816 Loss 1.8255\n",
            "Epoch 1 Batch 817 Loss 1.5932\n",
            "Epoch 1 Batch 818 Loss 1.4327\n",
            "Epoch 1 Batch 819 Loss 1.7075\n",
            "Epoch 1 Batch 820 Loss 1.8610\n",
            "Epoch 1 Batch 821 Loss 1.8308\n",
            "Epoch 1 Batch 822 Loss 1.7181\n",
            "Epoch 1 Batch 823 Loss 1.4527\n",
            "Epoch 1 Batch 824 Loss 1.6540\n",
            "Epoch 1 Batch 825 Loss 1.7237\n",
            "Epoch 1 Batch 826 Loss 1.9130\n",
            "Epoch 1 Batch 827 Loss 1.5513\n",
            "Epoch 1 Batch 828 Loss 1.5571\n",
            "Epoch 1 Batch 829 Loss 1.5423\n",
            "Epoch 1 Batch 830 Loss 1.6133\n",
            "Epoch 1 Batch 831 Loss 1.5935\n",
            "Epoch 1 Batch 832 Loss 1.4795\n",
            "Epoch 1 Batch 833 Loss 1.5942\n",
            "Epoch 1 Batch 834 Loss 1.6745\n",
            "Epoch 1 Batch 835 Loss 1.7182\n",
            "Epoch 1 Batch 836 Loss 1.7021\n",
            "Epoch 1 Batch 837 Loss 1.7693\n",
            "Epoch 1 Batch 838 Loss 1.8262\n",
            "Epoch 1 Batch 839 Loss 1.7632\n",
            "Epoch 1 Batch 840 Loss 1.6741\n",
            "Epoch 1 Batch 841 Loss 1.6636\n",
            "Epoch 1 Batch 842 Loss 1.7021\n",
            "Epoch 1 Batch 843 Loss 1.7396\n",
            "Epoch 1 Batch 844 Loss 1.5418\n",
            "Epoch 1 Batch 845 Loss 1.6374\n",
            "Epoch 1 Batch 846 Loss 1.9805\n",
            "Epoch 1 Batch 847 Loss 1.6160\n",
            "Epoch 1 Batch 848 Loss 1.7551\n",
            "Epoch 1 Batch 849 Loss 1.7049\n",
            "Epoch 1 Batch 850 Loss 1.6132\n",
            "Epoch 1 Batch 851 Loss 1.6601\n",
            "Epoch 1 Batch 852 Loss 1.6003\n",
            "Epoch 1 Batch 853 Loss 1.8283\n",
            "Epoch 1 Batch 854 Loss 1.6834\n",
            "Epoch 1 Batch 855 Loss 1.8338\n",
            "Epoch 1 Batch 856 Loss 1.6692\n",
            "Epoch 1 Batch 857 Loss 1.7050\n",
            "Epoch 1 Batch 858 Loss 1.6286\n",
            "Epoch 1 Batch 859 Loss 1.6493\n",
            "Epoch 1 Batch 860 Loss 1.7464\n",
            "Epoch 1 Batch 861 Loss 1.7624\n",
            "Epoch 1 Batch 862 Loss 1.9018\n",
            "Epoch 1 Batch 863 Loss 1.5778\n",
            "Epoch 1 Batch 864 Loss 1.7140\n",
            "Epoch 1 Batch 865 Loss 1.6422\n",
            "Epoch 1 Batch 866 Loss 1.5106\n",
            "Epoch 1 Batch 867 Loss 1.6421\n",
            "Epoch 1 Batch 868 Loss 1.7431\n",
            "Epoch 1 Batch 869 Loss 1.5686\n",
            "Epoch 1 Batch 870 Loss 1.8160\n",
            "Epoch 1 Batch 871 Loss 1.7450\n",
            "Epoch 1 Batch 872 Loss 1.6606\n",
            "Epoch 1 Batch 873 Loss 1.4909\n",
            "Epoch 1 Batch 874 Loss 1.7745\n",
            "Epoch 1 Batch 875 Loss 2.0144\n",
            "Epoch 1 Batch 876 Loss 1.6895\n",
            "Epoch 1 Batch 877 Loss 1.7860\n",
            "Epoch 1 Batch 878 Loss 1.5742\n",
            "Epoch 1 Batch 879 Loss 1.5707\n",
            "Epoch 1 Batch 880 Loss 1.8602\n",
            "Epoch 1 Batch 881 Loss 1.6427\n",
            "Epoch 1 Batch 882 Loss 1.6466\n",
            "Epoch 1 Batch 883 Loss 1.6784\n",
            "Epoch 1 Batch 884 Loss 1.2536\n",
            "Epoch 1 Batch 885 Loss 1.8939\n",
            "Epoch 1 Batch 886 Loss 1.5400\n",
            "Epoch 1 Batch 887 Loss 1.3141\n",
            "Epoch 1 Batch 888 Loss 1.6452\n",
            "Epoch 1 Batch 889 Loss 1.7586\n",
            "Epoch 1 Batch 890 Loss 1.5646\n",
            "Epoch 1 Batch 891 Loss 1.8634\n",
            "Epoch 1 Batch 892 Loss 1.8284\n",
            "Epoch 1 Batch 893 Loss 1.7561\n",
            "Epoch 1 Batch 894 Loss 1.5340\n",
            "Epoch 1 Batch 895 Loss 1.7187\n",
            "Epoch 1 Batch 896 Loss 1.5791\n",
            "Epoch 1 Batch 897 Loss 1.5318\n",
            "Epoch 1 Batch 898 Loss 1.7659\n",
            "Epoch 1 Batch 899 Loss 1.8238\n",
            "Epoch 1 Batch 900 Loss 1.4707\n",
            "Epoch 1 Batch 901 Loss 1.7840\n",
            "Epoch 1 Batch 902 Loss 1.6159\n",
            "Epoch 1 Batch 903 Loss 1.6301\n",
            "Epoch 1 Batch 904 Loss 1.5648\n",
            "Epoch 1 Batch 905 Loss 1.4756\n",
            "Epoch 1 Batch 906 Loss 1.3397\n",
            "Epoch 1 Batch 907 Loss 1.7460\n",
            "Epoch 1 Batch 908 Loss 1.7056\n",
            "Epoch 1 Batch 909 Loss 1.6065\n",
            "Epoch 1 Batch 910 Loss 1.7273\n",
            "Epoch 1 Batch 911 Loss 1.6489\n",
            "Epoch 1 Batch 912 Loss 1.9271\n",
            "Epoch 1 Batch 913 Loss 1.5141\n",
            "Epoch 1 Batch 914 Loss 1.7760\n",
            "Epoch 1 Batch 915 Loss 1.8893\n",
            "Epoch 1 Batch 916 Loss 1.7512\n",
            "Epoch 1 Batch 917 Loss 1.6433\n",
            "Epoch 1 Batch 918 Loss 1.5248\n",
            "Epoch 1 Batch 919 Loss 1.3981\n",
            "Epoch 1 Batch 920 Loss 1.4056\n",
            "Epoch 1 Batch 921 Loss 1.8061\n",
            "Epoch 1 Batch 922 Loss 1.7311\n",
            "Epoch 1 Batch 923 Loss 1.7159\n",
            "Epoch 1 Batch 924 Loss 1.6239\n",
            "Epoch 1 Batch 925 Loss 1.5038\n",
            "Epoch 1 Batch 926 Loss 1.5652\n",
            "Epoch 1 Batch 927 Loss 1.6613\n",
            "Epoch 1 Batch 928 Loss 1.6025\n",
            "Epoch 1 Batch 929 Loss 1.5871\n",
            "Epoch 1 Batch 930 Loss 1.6600\n",
            "Epoch 1 Batch 931 Loss 1.9418\n",
            "Epoch 1 Batch 932 Loss 1.5895\n",
            "Epoch 1 Batch 933 Loss 1.8296\n",
            "Epoch 1 Batch 934 Loss 1.3990\n",
            "Epoch 1 Batch 935 Loss 1.7230\n",
            "Epoch 1 Batch 936 Loss 1.7019\n",
            "Epoch 1 Batch 937 Loss 1.5030\n",
            "Epoch 1 Batch 938 Loss 1.7846\n",
            "Epoch 1 Batch 939 Loss 1.5929\n",
            "Epoch 1 Batch 940 Loss 1.8331\n",
            "Epoch 1 Batch 941 Loss 1.4922\n",
            "Epoch 1 Batch 942 Loss 1.5601\n",
            "Epoch 1 Batch 943 Loss 1.6610\n",
            "Epoch 1 Batch 944 Loss 1.6597\n",
            "Epoch 1 Batch 945 Loss 1.9104\n",
            "Epoch 1 Batch 946 Loss 1.6005\n",
            "Epoch 1 Batch 947 Loss 1.8360\n",
            "Epoch 1 Batch 948 Loss 1.8882\n",
            "Epoch 1 Batch 949 Loss 1.7964\n",
            "Epoch 1 Batch 950 Loss 1.5527\n",
            "Epoch 1 Batch 951 Loss 1.6109\n",
            "Epoch 1 Batch 952 Loss 1.4437\n",
            "Epoch 1 Batch 953 Loss 1.8168\n",
            "Epoch 1 Batch 954 Loss 1.8269\n",
            "Epoch 1 Batch 955 Loss 1.6952\n",
            "Epoch 1 Batch 956 Loss 1.6117\n",
            "Epoch 1 Batch 957 Loss 1.8335\n",
            "Epoch 1 Batch 958 Loss 1.5712\n",
            "Epoch 1 Batch 959 Loss 1.7838\n",
            "Epoch 1 Batch 960 Loss 1.5091\n",
            "Epoch 1 Batch 961 Loss 1.7180\n",
            "Epoch 1 Batch 962 Loss 1.4137\n",
            "Epoch 1 Batch 963 Loss 1.3362\n",
            "Epoch 1 Batch 964 Loss 1.5896\n",
            "Epoch 1 Batch 965 Loss 1.6472\n",
            "Epoch 1 Batch 966 Loss 1.5226\n",
            "Epoch 1 Batch 967 Loss 1.8047\n",
            "Epoch 1 Batch 968 Loss 1.7647\n",
            "Epoch 1 Batch 969 Loss 1.6032\n",
            "Epoch 1 Batch 970 Loss 1.7285\n",
            "Epoch 1 Batch 971 Loss 1.4841\n",
            "Epoch 1 Batch 972 Loss 1.6624\n",
            "Epoch 1 Batch 973 Loss 1.8138\n",
            "Epoch 1 Batch 974 Loss 1.2301\n",
            "Epoch 1 Batch 975 Loss 1.5694\n",
            "Epoch 1 Batch 976 Loss 1.4546\n",
            "Epoch 1 Batch 977 Loss 1.6092\n",
            "Epoch 1 Batch 978 Loss 1.6979\n",
            "Epoch 1 Batch 979 Loss 1.6199\n",
            "Epoch 1 Batch 980 Loss 1.4691\n",
            "Epoch 1 Batch 981 Loss 1.3567\n",
            "Epoch 1 Batch 982 Loss 1.3269\n",
            "Epoch 1 Batch 983 Loss 1.8879\n",
            "Epoch 1 Batch 984 Loss 1.8905\n",
            "Epoch 1 Batch 985 Loss 1.7369\n",
            "Epoch 1 Batch 986 Loss 1.8712\n",
            "Epoch 1 Batch 987 Loss 1.7198\n",
            "Epoch 1 Batch 988 Loss 1.4140\n",
            "Epoch 1 Batch 989 Loss 1.6104\n",
            "Epoch 1 Batch 990 Loss 1.6747\n",
            "Epoch 1 Batch 991 Loss 1.5404\n",
            "Epoch 1 Batch 992 Loss 1.5134\n",
            "Epoch 1 Batch 993 Loss 1.7366\n",
            "Epoch 1 Batch 994 Loss 1.8983\n",
            "Epoch 1 Batch 995 Loss 1.6656\n",
            "Epoch 1 Batch 996 Loss 1.5729\n",
            "Epoch 1 Batch 997 Loss 1.7982\n",
            "Epoch 1 Batch 998 Loss 1.5808\n",
            "Epoch 1 Batch 999 Loss 1.5528\n",
            "Epoch 1 Batch 1000 Loss 1.7520\n",
            "Epoch 1 Batch 1001 Loss 1.3563\n",
            "Epoch 1 Batch 1002 Loss 1.4918\n",
            "Epoch 1 Batch 1003 Loss 1.8159\n",
            "Epoch 1 Batch 1004 Loss 1.7352\n",
            "Epoch 1 Batch 1005 Loss 1.7665\n",
            "Epoch 1 Batch 1006 Loss 1.8305\n",
            "Epoch 1 Batch 1007 Loss 1.5634\n",
            "Epoch 1 Batch 1008 Loss 1.6922\n",
            "Epoch 1 Batch 1009 Loss 1.5540\n",
            "Epoch 1 Batch 1010 Loss 1.4990\n",
            "Epoch 1 Batch 1011 Loss 1.5739\n",
            "Epoch 1 Batch 1012 Loss 1.7632\n",
            "Epoch 1 Batch 1013 Loss 1.8187\n",
            "Epoch 1 Batch 1014 Loss 1.3434\n",
            "Epoch 1 Batch 1015 Loss 1.7178\n",
            "Epoch 1 Batch 1016 Loss 1.7269\n",
            "Epoch 1 Batch 1017 Loss 1.6726\n",
            "Epoch 1 Batch 1018 Loss 1.6220\n",
            "Epoch 1 Batch 1019 Loss 1.5576\n",
            "Epoch 1 Batch 1020 Loss 1.6630\n",
            "Epoch 1 Batch 1021 Loss 1.6889\n",
            "Epoch 1 Batch 1022 Loss 1.6049\n",
            "Epoch 1 Batch 1023 Loss 1.7718\n",
            "Epoch 1 Batch 1024 Loss 1.6910\n",
            "Epoch 1 Batch 1025 Loss 1.4464\n",
            "Epoch 1 Batch 1026 Loss 1.4754\n",
            "Epoch 1 Batch 1027 Loss 1.5448\n",
            "Epoch 1 Batch 1028 Loss 1.3472\n",
            "Epoch 1 Batch 1029 Loss 1.8708\n",
            "Epoch 1 Batch 1030 Loss 1.5702\n",
            "Epoch 1 Batch 1031 Loss 1.5927\n",
            "Epoch 1 Batch 1032 Loss 1.6428\n",
            "Epoch 1 Batch 1033 Loss 1.7812\n",
            "Epoch 1 Batch 1034 Loss 1.5391\n",
            "Epoch 1 Batch 1035 Loss 1.6938\n",
            "Epoch 1 Batch 1036 Loss 1.9213\n",
            "Epoch 1 Batch 1037 Loss 1.5083\n",
            "Epoch 1 Batch 1038 Loss 1.7201\n",
            "Epoch 1 Batch 1039 Loss 1.4125\n",
            "Epoch 1 Batch 1040 Loss 1.4424\n",
            "Epoch 1 Batch 1041 Loss 1.6282\n",
            "Epoch 1 Batch 1042 Loss 1.6275\n",
            "Epoch 1 Batch 1043 Loss 1.6312\n",
            "Epoch 1 Batch 1044 Loss 1.5763\n",
            "Epoch 1 Batch 1045 Loss 1.6393\n",
            "Epoch 1 Batch 1046 Loss 1.7192\n",
            "Epoch 1 Batch 1047 Loss 1.7129\n",
            "Epoch 1 Batch 1048 Loss 1.4721\n",
            "Epoch 1 Batch 1049 Loss 1.6412\n",
            "Epoch 1 Batch 1050 Loss 1.6038\n",
            "Epoch 1 Batch 1051 Loss 1.6337\n",
            "Epoch 1 Batch 1052 Loss 1.6640\n",
            "Epoch 1 Batch 1053 Loss 1.6673\n",
            "Epoch 1 Batch 1054 Loss 1.5334\n",
            "Epoch 1 Batch 1055 Loss 1.5154\n",
            "Epoch 1 Batch 1056 Loss 1.7038\n",
            "Epoch 1 Batch 1057 Loss 1.7078\n",
            "Epoch 1 Batch 1058 Loss 1.7017\n",
            "Epoch 1 Batch 1059 Loss 1.6630\n",
            "Epoch 1 Batch 1060 Loss 1.7777\n",
            "Epoch 1 Batch 1061 Loss 1.5385\n",
            "Epoch 1 Batch 1062 Loss 1.6009\n",
            "Epoch 1 Batch 1063 Loss 1.5634\n",
            "Epoch 1 Batch 1064 Loss 1.5901\n",
            "Epoch 1 Batch 1065 Loss 1.4500\n",
            "Epoch 1 Batch 1066 Loss 1.5016\n",
            "Epoch 1 Batch 1067 Loss 1.6335\n",
            "Epoch 1 Batch 1068 Loss 1.4525\n",
            "Epoch 1 Batch 1069 Loss 1.5242\n",
            "Epoch 1 Batch 1070 Loss 1.6097\n",
            "Epoch 1 Batch 1071 Loss 1.4190\n",
            "Epoch 1 Batch 1072 Loss 1.3932\n",
            "Epoch 1 Batch 1073 Loss 1.7946\n",
            "Epoch 1 Batch 1074 Loss 1.9103\n",
            "Epoch 1 Batch 1075 Loss 1.4197\n",
            "Epoch 1 Batch 1076 Loss 1.6235\n",
            "Epoch 1 Batch 1077 Loss 1.4852\n",
            "Epoch 1 Batch 1078 Loss 1.7763\n",
            "Epoch 1 Batch 1079 Loss 1.5300\n",
            "Epoch 1 Batch 1080 Loss 1.6578\n",
            "Epoch 1 Batch 1081 Loss 1.4568\n",
            "Epoch 1 Batch 1082 Loss 1.7989\n",
            "Epoch 1 Batch 1083 Loss 1.7004\n",
            "Epoch 1 Batch 1084 Loss 1.7073\n",
            "Epoch 1 Batch 1085 Loss 1.4798\n",
            "Epoch 1 Batch 1086 Loss 1.7216\n",
            "Epoch 1 Batch 1087 Loss 1.6165\n",
            "Epoch 1 Batch 1088 Loss 1.8058\n",
            "Epoch 1 Batch 1089 Loss 1.7874\n",
            "Epoch 1 Batch 1090 Loss 1.9192\n",
            "Epoch 1 Batch 1091 Loss 1.4082\n",
            "Epoch 1 Batch 1092 Loss 1.6379\n",
            "Epoch 1 Batch 1093 Loss 1.7889\n",
            "Epoch 1 Batch 1094 Loss 1.4806\n",
            "Epoch 1 Batch 1095 Loss 1.6495\n",
            "Epoch 1 Batch 1096 Loss 1.7425\n",
            "Epoch 1 Batch 1097 Loss 1.7977\n",
            "Epoch 1 Batch 1098 Loss 1.8003\n",
            "Epoch 1 Batch 1099 Loss 1.4769\n",
            "Epoch 1 Batch 1100 Loss 1.7823\n",
            "Epoch 1 Batch 1101 Loss 1.4309\n",
            "Epoch 1 Batch 1102 Loss 1.8322\n",
            "Epoch 1 Batch 1103 Loss 1.7940\n",
            "Epoch 1 Batch 1104 Loss 1.5912\n",
            "Epoch 1 Batch 1105 Loss 1.2921\n",
            "Epoch 1 Batch 1106 Loss 1.7288\n",
            "Epoch 1 Batch 1107 Loss 1.6138\n",
            "Epoch 1 Batch 1108 Loss 1.5854\n",
            "Epoch 1 Batch 1109 Loss 1.6302\n",
            "Epoch 1 Batch 1110 Loss 1.5791\n",
            "Epoch 1 Batch 1111 Loss 1.4450\n",
            "Epoch 1 Batch 1112 Loss 1.7752\n",
            "Epoch 1 Batch 1113 Loss 1.6566\n",
            "Epoch 1 Batch 1114 Loss 1.4565\n",
            "Epoch 1 Batch 1115 Loss 1.4177\n",
            "Epoch 1 Batch 1116 Loss 1.3912\n",
            "Epoch 1 Batch 1117 Loss 1.5509\n",
            "Epoch 1 Batch 1118 Loss 1.6364\n",
            "Epoch 1 Batch 1119 Loss 1.5150\n",
            "Epoch 1 Batch 1120 Loss 1.5488\n",
            "Epoch 1 Batch 1121 Loss 1.5423\n",
            "Epoch 1 Batch 1122 Loss 1.8028\n",
            "Epoch 1 Batch 1123 Loss 1.5365\n",
            "Epoch 1 Batch 1124 Loss 1.7798\n",
            "Epoch 1 Batch 1125 Loss 1.7634\n",
            "Epoch 1 Batch 1126 Loss 1.5676\n",
            "Epoch 1 Batch 1127 Loss 1.4886\n",
            "Epoch 1 Batch 1128 Loss 1.7504\n",
            "Epoch 1 Batch 1129 Loss 1.6168\n",
            "Epoch 1 Batch 1130 Loss 1.4978\n",
            "Epoch 1 Batch 1131 Loss 1.6331\n",
            "Epoch 1 Batch 1132 Loss 1.5795\n",
            "Epoch 1 Batch 1133 Loss 1.4941\n",
            "Epoch 1 Batch 1134 Loss 1.5691\n",
            "Epoch 1 Batch 1135 Loss 1.7888\n",
            "Epoch 1 Batch 1136 Loss 1.6795\n",
            "Epoch 1 Batch 1137 Loss 1.6402\n",
            "Epoch 1 Batch 1138 Loss 1.8191\n",
            "Epoch 1 Batch 1139 Loss 1.6890\n",
            "Epoch 1 Batch 1140 Loss 1.5423\n",
            "Epoch 1 Batch 1141 Loss 1.5778\n",
            "Epoch 1 Batch 1142 Loss 1.7653\n",
            "Epoch 1 Batch 1143 Loss 1.7642\n",
            "Epoch 1 Batch 1144 Loss 1.6015\n",
            "Epoch 1 Batch 1145 Loss 1.6049\n",
            "Epoch 1 Batch 1146 Loss 1.4689\n",
            "Epoch 1 Batch 1147 Loss 1.5290\n",
            "Epoch 1 Batch 1148 Loss 1.4436\n",
            "Epoch 1 Batch 1149 Loss 1.4866\n",
            "Epoch 1 Batch 1150 Loss 1.5980\n",
            "Epoch 1 Batch 1151 Loss 1.3465\n",
            "Epoch 1 Batch 1152 Loss 1.5782\n",
            "Epoch 1 Batch 1153 Loss 1.5681\n",
            "Epoch 1 Batch 1154 Loss 1.5283\n",
            "Epoch 1 Batch 1155 Loss 1.7567\n",
            "Epoch 1 Batch 1156 Loss 1.3887\n",
            "Epoch 1 Batch 1157 Loss 1.6766\n",
            "Epoch 1 Batch 1158 Loss 1.6534\n",
            "Epoch 1 Batch 1159 Loss 1.4108\n",
            "Epoch 1 Batch 1160 Loss 1.6533\n",
            "Epoch 1 Batch 1161 Loss 1.8702\n",
            "Epoch 1 Batch 1162 Loss 1.5098\n",
            "Epoch 1 Batch 1163 Loss 1.5316\n",
            "Epoch 1 Batch 1164 Loss 1.6518\n",
            "Epoch 1 Batch 1165 Loss 1.5816\n",
            "Epoch 1 Batch 1166 Loss 1.6841\n",
            "Epoch 1 Batch 1167 Loss 1.5632\n",
            "Epoch 1 Batch 1168 Loss 1.7725\n",
            "Epoch 1 Batch 1169 Loss 1.6062\n",
            "Epoch 1 Batch 1170 Loss 1.5622\n",
            "Epoch 1 Batch 1171 Loss 1.6470\n",
            "Epoch 1 Batch 1172 Loss 1.5735\n",
            "Epoch 1 Batch 1173 Loss 1.6510\n",
            "Epoch 1 Batch 1174 Loss 1.6758\n",
            "Epoch 1 Batch 1175 Loss 1.8568\n",
            "Epoch 1 Batch 1176 Loss 1.4839\n",
            "Epoch 1 Batch 1177 Loss 1.4395\n",
            "Epoch 1 Batch 1178 Loss 1.5515\n",
            "Epoch 1 Batch 1179 Loss 1.3659\n",
            "Epoch 1 Batch 1180 Loss 1.5946\n",
            "Epoch 1 Batch 1181 Loss 1.5966\n",
            "Epoch 1 Batch 1182 Loss 1.7180\n",
            "Epoch 1 Batch 1183 Loss 1.7374\n",
            "Epoch 1 Batch 1184 Loss 1.5148\n",
            "Epoch 1 Batch 1185 Loss 1.7067\n",
            "Epoch 1 Batch 1186 Loss 1.7894\n",
            "Epoch 1 Batch 1187 Loss 1.5195\n",
            "Epoch 1 Batch 1188 Loss 1.5551\n",
            "Epoch 1 Batch 1189 Loss 1.6259\n",
            "Epoch 1 Batch 1190 Loss 1.6092\n",
            "Epoch 1 Batch 1191 Loss 1.7853\n",
            "Epoch 1 Batch 1192 Loss 1.5507\n",
            "Epoch 1 Batch 1193 Loss 1.6267\n",
            "Epoch 1 Batch 1194 Loss 1.4295\n",
            "Epoch 1 Batch 1195 Loss 1.7538\n",
            "Epoch 1 Batch 1196 Loss 1.6803\n",
            "Epoch 1 Batch 1197 Loss 1.4929\n",
            "Epoch 1 Batch 1198 Loss 1.6124\n",
            "Epoch 1 Batch 1199 Loss 1.5389\n",
            "Epoch 1 Batch 1200 Loss 1.6802\n",
            "Epoch 1 Batch 1201 Loss 1.5180\n",
            "Epoch 1 Batch 1202 Loss 1.7944\n",
            "Epoch 1 Batch 1203 Loss 1.7494\n",
            "Epoch 1 Batch 1204 Loss 1.6365\n",
            "Epoch 1 Batch 1205 Loss 1.3832\n",
            "Epoch 1 Batch 1206 Loss 1.7460\n",
            "Epoch 1 Batch 1207 Loss 1.4221\n",
            "Epoch 1 Batch 1208 Loss 1.6739\n",
            "Epoch 1 Batch 1209 Loss 1.7947\n",
            "Epoch 1 Batch 1210 Loss 1.3572\n",
            "Epoch 1 Batch 1211 Loss 1.6031\n",
            "Epoch 1 Batch 1212 Loss 1.5666\n",
            "Epoch 1 Batch 1213 Loss 1.6271\n",
            "Epoch 1 Batch 1214 Loss 1.4292\n",
            "Epoch 1 Batch 1215 Loss 1.7671\n",
            "Epoch 1 Batch 1216 Loss 1.4082\n",
            "Epoch 1 Batch 1217 Loss 1.4059\n",
            "Epoch 1 Batch 1218 Loss 1.7454\n",
            "Epoch 1 Batch 1219 Loss 1.4047\n",
            "Epoch 1 Batch 1220 Loss 1.7303\n",
            "Epoch 1 Batch 1221 Loss 1.3412\n",
            "Epoch 1 Batch 1222 Loss 1.8295\n",
            "Epoch 1 Batch 1223 Loss 1.7408\n",
            "Epoch 1 Batch 1224 Loss 1.7785\n",
            "Epoch 1 Batch 1225 Loss 1.6596\n",
            "Epoch 1 Batch 1226 Loss 1.3837\n",
            "Epoch 1 Batch 1227 Loss 1.4991\n",
            "Epoch 1 Batch 1228 Loss 1.2532\n",
            "Epoch 1 Batch 1229 Loss 1.5826\n",
            "Epoch 1 Batch 1230 Loss 1.5849\n",
            "Epoch 1 Batch 1231 Loss 1.6754\n",
            "Epoch 1 Batch 1232 Loss 1.7004\n",
            "Epoch 1 Batch 1233 Loss 1.6104\n",
            "Epoch 1 Batch 1234 Loss 1.7469\n",
            "Epoch 1 Batch 1235 Loss 1.6006\n",
            "Epoch 1 Batch 1236 Loss 1.5251\n",
            "Epoch 1 Batch 1237 Loss 1.3727\n",
            "Epoch 1 Batch 1238 Loss 1.7586\n",
            "Epoch 1 Batch 1239 Loss 1.5880\n",
            "Epoch 1 Batch 1240 Loss 1.2497\n",
            "Epoch 1 Batch 1241 Loss 1.8860\n",
            "Epoch 1 Batch 1242 Loss 1.6788\n",
            "Epoch 1 Batch 1243 Loss 1.3418\n",
            "Epoch 1 Batch 1244 Loss 1.5823\n",
            "Epoch 1 Batch 1245 Loss 1.5265\n",
            "Epoch 1 Batch 1246 Loss 1.7077\n",
            "Epoch 1 Batch 1247 Loss 1.7127\n",
            "Epoch 1 Batch 1248 Loss 1.5170\n",
            "Epoch 1 Batch 1249 Loss 1.3416\n",
            "Epoch 1 Batch 1250 Loss 1.4596\n",
            "Epoch 1 Batch 1251 Loss 1.5068\n",
            "Epoch 1 Batch 1252 Loss 1.4876\n",
            "Epoch 1 Batch 1253 Loss 1.6283\n",
            "Epoch 1 Batch 1254 Loss 1.5497\n",
            "Epoch 1 Batch 1255 Loss 1.7184\n",
            "Epoch 1 Batch 1256 Loss 1.5049\n",
            "Epoch 1 Batch 1257 Loss 1.7516\n",
            "Epoch 1 Batch 1258 Loss 1.2585\n",
            "Epoch 1 Batch 1259 Loss 1.7012\n",
            "Epoch 1 Batch 1260 Loss 1.3415\n",
            "Epoch 1 Batch 1261 Loss 1.6289\n",
            "Epoch 1 Batch 1262 Loss 1.6470\n",
            "Epoch 1 Batch 1263 Loss 1.7341\n",
            "Epoch 1 Batch 1264 Loss 1.4234\n",
            "Epoch 1 Batch 1265 Loss 1.6449\n",
            "Epoch 1 Batch 1266 Loss 1.7583\n",
            "Epoch 1 Batch 1267 Loss 1.4942\n",
            "Epoch 1 Batch 1268 Loss 1.3223\n",
            "Epoch 1 Batch 1269 Loss 1.5186\n",
            "Epoch 1 Batch 1270 Loss 1.3632\n",
            "Epoch 1 Batch 1271 Loss 1.5688\n",
            "Epoch 1 Batch 1272 Loss 1.6164\n",
            "Epoch 1 Batch 1273 Loss 1.3816\n",
            "Epoch 1 Loss 1.8873\n",
            "Time taken for 1 epoch 1673.9892466068268 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4418\n",
            "Epoch 2 Batch 1 Loss 1.4570\n",
            "Epoch 2 Batch 2 Loss 1.5287\n",
            "Epoch 2 Batch 3 Loss 1.4745\n",
            "Epoch 2 Batch 4 Loss 1.3858\n",
            "Epoch 2 Batch 5 Loss 1.2797\n",
            "Epoch 2 Batch 6 Loss 1.6036\n",
            "Epoch 2 Batch 7 Loss 1.5880\n",
            "Epoch 2 Batch 8 Loss 1.4443\n",
            "Epoch 2 Batch 9 Loss 1.6310\n",
            "Epoch 2 Batch 10 Loss 1.6471\n",
            "Epoch 2 Batch 11 Loss 1.5727\n",
            "Epoch 2 Batch 12 Loss 1.4526\n",
            "Epoch 2 Batch 13 Loss 1.3285\n",
            "Epoch 2 Batch 14 Loss 1.4029\n",
            "Epoch 2 Batch 15 Loss 1.5671\n",
            "Epoch 2 Batch 16 Loss 1.2494\n",
            "Epoch 2 Batch 17 Loss 1.5004\n",
            "Epoch 2 Batch 18 Loss 1.2558\n",
            "Epoch 2 Batch 19 Loss 1.5606\n",
            "Epoch 2 Batch 20 Loss 1.3147\n",
            "Epoch 2 Batch 21 Loss 1.2893\n",
            "Epoch 2 Batch 22 Loss 1.1898\n",
            "Epoch 2 Batch 23 Loss 1.5279\n",
            "Epoch 2 Batch 24 Loss 1.2819\n",
            "Epoch 2 Batch 25 Loss 1.5767\n",
            "Epoch 2 Batch 26 Loss 1.3444\n",
            "Epoch 2 Batch 27 Loss 1.3540\n",
            "Epoch 2 Batch 28 Loss 1.3180\n",
            "Epoch 2 Batch 29 Loss 1.3300\n",
            "Epoch 2 Batch 30 Loss 1.5271\n",
            "Epoch 2 Batch 31 Loss 1.3561\n",
            "Epoch 2 Batch 32 Loss 1.6453\n",
            "Epoch 2 Batch 33 Loss 1.4443\n",
            "Epoch 2 Batch 34 Loss 1.5173\n",
            "Epoch 2 Batch 35 Loss 1.4422\n",
            "Epoch 2 Batch 36 Loss 1.3823\n",
            "Epoch 2 Batch 37 Loss 1.5012\n",
            "Epoch 2 Batch 38 Loss 1.5266\n",
            "Epoch 2 Batch 39 Loss 1.4506\n",
            "Epoch 2 Batch 40 Loss 1.4288\n",
            "Epoch 2 Batch 41 Loss 1.2345\n",
            "Epoch 2 Batch 42 Loss 1.5977\n",
            "Epoch 2 Batch 43 Loss 1.4306\n",
            "Epoch 2 Batch 44 Loss 1.2622\n",
            "Epoch 2 Batch 45 Loss 1.5741\n",
            "Epoch 2 Batch 46 Loss 1.3335\n",
            "Epoch 2 Batch 47 Loss 1.4899\n",
            "Epoch 2 Batch 48 Loss 1.6909\n",
            "Epoch 2 Batch 49 Loss 1.5843\n",
            "Epoch 2 Batch 50 Loss 1.5207\n",
            "Epoch 2 Batch 51 Loss 1.2892\n",
            "Epoch 2 Batch 52 Loss 1.4251\n",
            "Epoch 2 Batch 53 Loss 1.3052\n",
            "Epoch 2 Batch 54 Loss 1.4438\n",
            "Epoch 2 Batch 55 Loss 1.4245\n",
            "Epoch 2 Batch 56 Loss 1.2354\n",
            "Epoch 2 Batch 57 Loss 1.5926\n",
            "Epoch 2 Batch 58 Loss 1.4071\n",
            "Epoch 2 Batch 59 Loss 1.7240\n",
            "Epoch 2 Batch 60 Loss 1.5863\n",
            "Epoch 2 Batch 61 Loss 1.3798\n",
            "Epoch 2 Batch 62 Loss 1.6399\n",
            "Epoch 2 Batch 63 Loss 1.6859\n",
            "Epoch 2 Batch 64 Loss 1.5942\n",
            "Epoch 2 Batch 65 Loss 1.5494\n",
            "Epoch 2 Batch 66 Loss 1.5789\n",
            "Epoch 2 Batch 67 Loss 1.4139\n",
            "Epoch 2 Batch 68 Loss 1.6039\n",
            "Epoch 2 Batch 69 Loss 1.3258\n",
            "Epoch 2 Batch 70 Loss 1.3048\n",
            "Epoch 2 Batch 71 Loss 1.2721\n",
            "Epoch 2 Batch 72 Loss 1.5504\n",
            "Epoch 2 Batch 73 Loss 1.5976\n",
            "Epoch 2 Batch 74 Loss 1.3351\n",
            "Epoch 2 Batch 75 Loss 1.2605\n",
            "Epoch 2 Batch 76 Loss 1.4334\n",
            "Epoch 2 Batch 77 Loss 1.5150\n",
            "Epoch 2 Batch 78 Loss 1.5239\n",
            "Epoch 2 Batch 79 Loss 1.3956\n",
            "Epoch 2 Batch 80 Loss 1.3276\n",
            "Epoch 2 Batch 81 Loss 1.6286\n",
            "Epoch 2 Batch 82 Loss 1.3626\n",
            "Epoch 2 Batch 83 Loss 1.5725\n",
            "Epoch 2 Batch 84 Loss 1.2522\n",
            "Epoch 2 Batch 85 Loss 1.3514\n",
            "Epoch 2 Batch 86 Loss 1.4776\n",
            "Epoch 2 Batch 87 Loss 1.2004\n",
            "Epoch 2 Batch 88 Loss 1.3428\n",
            "Epoch 2 Batch 89 Loss 1.3931\n",
            "Epoch 2 Batch 90 Loss 1.5305\n",
            "Epoch 2 Batch 91 Loss 1.3527\n",
            "Epoch 2 Batch 92 Loss 1.7163\n",
            "Epoch 2 Batch 93 Loss 1.4880\n",
            "Epoch 2 Batch 94 Loss 1.7020\n",
            "Epoch 2 Batch 95 Loss 1.4713\n",
            "Epoch 2 Batch 96 Loss 1.6477\n",
            "Epoch 2 Batch 97 Loss 1.4654\n",
            "Epoch 2 Batch 98 Loss 1.5900\n",
            "Epoch 2 Batch 99 Loss 1.5631\n",
            "Epoch 2 Batch 100 Loss 1.5860\n",
            "Epoch 2 Batch 101 Loss 1.4572\n",
            "Epoch 2 Batch 102 Loss 1.5502\n",
            "Epoch 2 Batch 103 Loss 1.6193\n",
            "Epoch 2 Batch 104 Loss 1.4161\n",
            "Epoch 2 Batch 105 Loss 1.4390\n",
            "Epoch 2 Batch 106 Loss 1.2733\n",
            "Epoch 2 Batch 107 Loss 1.1008\n",
            "Epoch 2 Batch 108 Loss 1.5995\n",
            "Epoch 2 Batch 109 Loss 1.2199\n",
            "Epoch 2 Batch 110 Loss 1.5528\n",
            "Epoch 2 Batch 111 Loss 1.3587\n",
            "Epoch 2 Batch 112 Loss 1.4392\n",
            "Epoch 2 Batch 113 Loss 1.1948\n",
            "Epoch 2 Batch 114 Loss 1.4942\n",
            "Epoch 2 Batch 115 Loss 1.4718\n",
            "Epoch 2 Batch 116 Loss 1.3550\n",
            "Epoch 2 Batch 117 Loss 1.6916\n",
            "Epoch 2 Batch 118 Loss 1.5971\n",
            "Epoch 2 Batch 119 Loss 1.5166\n",
            "Epoch 2 Batch 120 Loss 1.3059\n",
            "Epoch 2 Batch 121 Loss 1.6206\n",
            "Epoch 2 Batch 122 Loss 1.4266\n",
            "Epoch 2 Batch 123 Loss 1.3138\n",
            "Epoch 2 Batch 124 Loss 1.6496\n",
            "Epoch 2 Batch 125 Loss 1.5013\n",
            "Epoch 2 Batch 126 Loss 1.4462\n",
            "Epoch 2 Batch 127 Loss 1.4276\n",
            "Epoch 2 Batch 128 Loss 1.5290\n",
            "Epoch 2 Batch 129 Loss 1.5477\n",
            "Epoch 2 Batch 130 Loss 1.5074\n",
            "Epoch 2 Batch 131 Loss 1.2289\n",
            "Epoch 2 Batch 132 Loss 1.3751\n",
            "Epoch 2 Batch 133 Loss 1.3282\n",
            "Epoch 2 Batch 134 Loss 1.5734\n",
            "Epoch 2 Batch 135 Loss 1.5859\n",
            "Epoch 2 Batch 136 Loss 1.2537\n",
            "Epoch 2 Batch 137 Loss 1.4868\n",
            "Epoch 2 Batch 138 Loss 1.5019\n",
            "Epoch 2 Batch 139 Loss 1.5731\n",
            "Epoch 2 Batch 140 Loss 1.3720\n",
            "Epoch 2 Batch 141 Loss 1.3081\n",
            "Epoch 2 Batch 142 Loss 1.5335\n",
            "Epoch 2 Batch 143 Loss 1.6204\n",
            "Epoch 2 Batch 144 Loss 1.6544\n",
            "Epoch 2 Batch 145 Loss 1.3029\n",
            "Epoch 2 Batch 146 Loss 1.4392\n",
            "Epoch 2 Batch 147 Loss 1.5397\n",
            "Epoch 2 Batch 148 Loss 1.2881\n",
            "Epoch 2 Batch 149 Loss 1.5262\n",
            "Epoch 2 Batch 150 Loss 1.4552\n",
            "Epoch 2 Batch 151 Loss 1.5463\n",
            "Epoch 2 Batch 152 Loss 1.4752\n",
            "Epoch 2 Batch 153 Loss 1.3968\n",
            "Epoch 2 Batch 154 Loss 1.3484\n",
            "Epoch 2 Batch 155 Loss 1.4658\n",
            "Epoch 2 Batch 156 Loss 1.5358\n",
            "Epoch 2 Batch 157 Loss 1.2949\n",
            "Epoch 2 Batch 158 Loss 1.6135\n",
            "Epoch 2 Batch 159 Loss 1.3430\n",
            "Epoch 2 Batch 160 Loss 1.4026\n",
            "Epoch 2 Batch 161 Loss 1.3073\n",
            "Epoch 2 Batch 162 Loss 1.2920\n",
            "Epoch 2 Batch 163 Loss 1.5547\n",
            "Epoch 2 Batch 164 Loss 1.5218\n",
            "Epoch 2 Batch 165 Loss 1.5051\n",
            "Epoch 2 Batch 166 Loss 1.3476\n",
            "Epoch 2 Batch 167 Loss 1.4225\n",
            "Epoch 2 Batch 168 Loss 1.5921\n",
            "Epoch 2 Batch 169 Loss 1.3612\n",
            "Epoch 2 Batch 170 Loss 1.3891\n",
            "Epoch 2 Batch 171 Loss 1.3867\n",
            "Epoch 2 Batch 172 Loss 1.4949\n",
            "Epoch 2 Batch 173 Loss 1.3064\n",
            "Epoch 2 Batch 174 Loss 1.5460\n",
            "Epoch 2 Batch 175 Loss 1.3796\n",
            "Epoch 2 Batch 176 Loss 1.1261\n",
            "Epoch 2 Batch 177 Loss 1.3212\n",
            "Epoch 2 Batch 178 Loss 1.4477\n",
            "Epoch 2 Batch 179 Loss 1.4966\n",
            "Epoch 2 Batch 180 Loss 1.1730\n",
            "Epoch 2 Batch 181 Loss 1.4571\n",
            "Epoch 2 Batch 182 Loss 1.5359\n",
            "Epoch 2 Batch 183 Loss 1.5279\n",
            "Epoch 2 Batch 184 Loss 1.4811\n",
            "Epoch 2 Batch 185 Loss 1.3587\n",
            "Epoch 2 Batch 186 Loss 1.1104\n",
            "Epoch 2 Batch 187 Loss 1.3544\n",
            "Epoch 2 Batch 188 Loss 1.3468\n",
            "Epoch 2 Batch 189 Loss 1.4310\n",
            "Epoch 2 Batch 190 Loss 1.2620\n",
            "Epoch 2 Batch 191 Loss 1.4023\n",
            "Epoch 2 Batch 192 Loss 1.2522\n",
            "Epoch 2 Batch 193 Loss 1.4119\n",
            "Epoch 2 Batch 194 Loss 1.4475\n",
            "Epoch 2 Batch 195 Loss 1.2998\n",
            "Epoch 2 Batch 196 Loss 1.4340\n",
            "Epoch 2 Batch 197 Loss 1.6281\n",
            "Epoch 2 Batch 198 Loss 1.5008\n",
            "Epoch 2 Batch 199 Loss 1.1352\n",
            "Epoch 2 Batch 200 Loss 1.7103\n",
            "Epoch 2 Batch 201 Loss 1.5937\n",
            "Epoch 2 Batch 202 Loss 1.2157\n",
            "Epoch 2 Batch 203 Loss 1.3956\n",
            "Epoch 2 Batch 204 Loss 1.3984\n",
            "Epoch 2 Batch 205 Loss 1.4917\n",
            "Epoch 2 Batch 206 Loss 1.3578\n",
            "Epoch 2 Batch 207 Loss 1.3087\n",
            "Epoch 2 Batch 208 Loss 1.5129\n",
            "Epoch 2 Batch 209 Loss 1.2747\n",
            "Epoch 2 Batch 210 Loss 1.6009\n",
            "Epoch 2 Batch 211 Loss 1.6004\n",
            "Epoch 2 Batch 212 Loss 1.4713\n",
            "Epoch 2 Batch 213 Loss 1.3503\n",
            "Epoch 2 Batch 214 Loss 1.6051\n",
            "Epoch 2 Batch 215 Loss 1.5939\n",
            "Epoch 2 Batch 216 Loss 1.5944\n",
            "Epoch 2 Batch 217 Loss 1.3724\n",
            "Epoch 2 Batch 218 Loss 1.4325\n",
            "Epoch 2 Batch 219 Loss 1.7520\n",
            "Epoch 2 Batch 220 Loss 1.4590\n",
            "Epoch 2 Batch 221 Loss 1.3171\n",
            "Epoch 2 Batch 222 Loss 1.4200\n",
            "Epoch 2 Batch 223 Loss 1.4133\n",
            "Epoch 2 Batch 224 Loss 1.3235\n",
            "Epoch 2 Batch 225 Loss 1.3644\n",
            "Epoch 2 Batch 226 Loss 1.8077\n",
            "Epoch 2 Batch 227 Loss 1.5393\n",
            "Epoch 2 Batch 228 Loss 1.3880\n",
            "Epoch 2 Batch 229 Loss 1.6034\n",
            "Epoch 2 Batch 230 Loss 1.4915\n",
            "Epoch 2 Batch 231 Loss 1.5848\n",
            "Epoch 2 Batch 232 Loss 1.4789\n",
            "Epoch 2 Batch 233 Loss 1.6119\n",
            "Epoch 2 Batch 234 Loss 1.4733\n",
            "Epoch 2 Batch 235 Loss 1.5513\n",
            "Epoch 2 Batch 236 Loss 1.5944\n",
            "Epoch 2 Batch 237 Loss 1.3791\n",
            "Epoch 2 Batch 238 Loss 1.2022\n",
            "Epoch 2 Batch 239 Loss 1.3737\n",
            "Epoch 2 Batch 240 Loss 1.5364\n",
            "Epoch 2 Batch 241 Loss 1.6774\n",
            "Epoch 2 Batch 242 Loss 1.6861\n",
            "Epoch 2 Batch 243 Loss 1.4626\n",
            "Epoch 2 Batch 244 Loss 1.2659\n",
            "Epoch 2 Batch 245 Loss 1.4573\n",
            "Epoch 2 Batch 246 Loss 1.6467\n",
            "Epoch 2 Batch 247 Loss 1.6007\n",
            "Epoch 2 Batch 248 Loss 1.5150\n",
            "Epoch 2 Batch 249 Loss 1.7101\n",
            "Epoch 2 Batch 250 Loss 1.6266\n",
            "Epoch 2 Batch 251 Loss 1.5768\n",
            "Epoch 2 Batch 252 Loss 1.5167\n",
            "Epoch 2 Batch 253 Loss 1.4247\n",
            "Epoch 2 Batch 254 Loss 1.3732\n",
            "Epoch 2 Batch 255 Loss 1.3415\n",
            "Epoch 2 Batch 256 Loss 1.5229\n",
            "Epoch 2 Batch 257 Loss 1.4354\n",
            "Epoch 2 Batch 258 Loss 1.4415\n",
            "Epoch 2 Batch 259 Loss 1.4276\n",
            "Epoch 2 Batch 260 Loss 1.2549\n",
            "Epoch 2 Batch 261 Loss 1.6018\n",
            "Epoch 2 Batch 262 Loss 1.3862\n",
            "Epoch 2 Batch 263 Loss 1.5017\n",
            "Epoch 2 Batch 264 Loss 1.3840\n",
            "Epoch 2 Batch 265 Loss 1.6189\n",
            "Epoch 2 Batch 266 Loss 1.4411\n",
            "Epoch 2 Batch 267 Loss 1.4684\n",
            "Epoch 2 Batch 268 Loss 1.5077\n",
            "Epoch 2 Batch 269 Loss 1.2737\n",
            "Epoch 2 Batch 270 Loss 1.5174\n",
            "Epoch 2 Batch 271 Loss 1.2664\n",
            "Epoch 2 Batch 272 Loss 1.2318\n",
            "Epoch 2 Batch 273 Loss 1.3546\n",
            "Epoch 2 Batch 274 Loss 1.3910\n",
            "Epoch 2 Batch 275 Loss 1.4199\n",
            "Epoch 2 Batch 276 Loss 1.2274\n",
            "Epoch 2 Batch 277 Loss 1.8066\n",
            "Epoch 2 Batch 278 Loss 1.2393\n",
            "Epoch 2 Batch 279 Loss 1.4663\n",
            "Epoch 2 Batch 280 Loss 1.5591\n",
            "Epoch 2 Batch 281 Loss 1.5431\n",
            "Epoch 2 Batch 282 Loss 1.6261\n",
            "Epoch 2 Batch 283 Loss 1.3569\n",
            "Epoch 2 Batch 284 Loss 1.3841\n",
            "Epoch 2 Batch 285 Loss 1.4518\n",
            "Epoch 2 Batch 286 Loss 1.3396\n",
            "Epoch 2 Batch 287 Loss 1.4840\n",
            "Epoch 2 Batch 288 Loss 1.5393\n",
            "Epoch 2 Batch 289 Loss 1.4925\n",
            "Epoch 2 Batch 290 Loss 1.3866\n",
            "Epoch 2 Batch 291 Loss 1.7172\n",
            "Epoch 2 Batch 292 Loss 1.5424\n",
            "Epoch 2 Batch 293 Loss 1.5821\n",
            "Epoch 2 Batch 294 Loss 1.4615\n",
            "Epoch 2 Batch 295 Loss 1.4702\n",
            "Epoch 2 Batch 296 Loss 1.4137\n",
            "Epoch 2 Batch 297 Loss 1.4891\n",
            "Epoch 2 Batch 298 Loss 1.4125\n",
            "Epoch 2 Batch 299 Loss 1.3118\n",
            "Epoch 2 Batch 300 Loss 1.3007\n",
            "Epoch 2 Batch 301 Loss 1.6507\n",
            "Epoch 2 Batch 302 Loss 1.6234\n",
            "Epoch 2 Batch 303 Loss 1.2736\n",
            "Epoch 2 Batch 304 Loss 1.3676\n",
            "Epoch 2 Batch 305 Loss 1.3253\n",
            "Epoch 2 Batch 306 Loss 1.5333\n",
            "Epoch 2 Batch 307 Loss 1.5650\n",
            "Epoch 2 Batch 308 Loss 1.5027\n",
            "Epoch 2 Batch 309 Loss 1.3428\n",
            "Epoch 2 Batch 310 Loss 1.3845\n",
            "Epoch 2 Batch 311 Loss 1.3577\n",
            "Epoch 2 Batch 312 Loss 1.5005\n",
            "Epoch 2 Batch 313 Loss 1.3789\n",
            "Epoch 2 Batch 314 Loss 1.3074\n",
            "Epoch 2 Batch 315 Loss 1.5074\n",
            "Epoch 2 Batch 316 Loss 1.5996\n",
            "Epoch 2 Batch 317 Loss 1.4063\n",
            "Epoch 2 Batch 318 Loss 1.3376\n",
            "Epoch 2 Batch 319 Loss 1.4254\n",
            "Epoch 2 Batch 320 Loss 1.4124\n",
            "Epoch 2 Batch 321 Loss 1.5709\n",
            "Epoch 2 Batch 322 Loss 1.4219\n",
            "Epoch 2 Batch 323 Loss 1.5375\n",
            "Epoch 2 Batch 324 Loss 1.7200\n",
            "Epoch 2 Batch 325 Loss 1.3448\n",
            "Epoch 2 Batch 326 Loss 1.6307\n",
            "Epoch 2 Batch 327 Loss 1.2750\n",
            "Epoch 2 Batch 328 Loss 1.4598\n",
            "Epoch 2 Batch 329 Loss 1.2963\n",
            "Epoch 2 Batch 330 Loss 1.1194\n",
            "Epoch 2 Batch 331 Loss 1.4646\n",
            "Epoch 2 Batch 332 Loss 1.3945\n",
            "Epoch 2 Batch 333 Loss 1.5751\n",
            "Epoch 2 Batch 334 Loss 1.3995\n",
            "Epoch 2 Batch 335 Loss 1.5060\n",
            "Epoch 2 Batch 336 Loss 1.3518\n",
            "Epoch 2 Batch 337 Loss 1.3014\n",
            "Epoch 2 Batch 338 Loss 1.5730\n",
            "Epoch 2 Batch 339 Loss 1.5700\n",
            "Epoch 2 Batch 340 Loss 1.4137\n",
            "Epoch 2 Batch 341 Loss 1.4852\n",
            "Epoch 2 Batch 342 Loss 1.4654\n",
            "Epoch 2 Batch 343 Loss 1.4496\n",
            "Epoch 2 Batch 344 Loss 1.3668\n",
            "Epoch 2 Batch 345 Loss 1.3597\n",
            "Epoch 2 Batch 346 Loss 1.4698\n",
            "Epoch 2 Batch 347 Loss 1.6032\n",
            "Epoch 2 Batch 348 Loss 1.4310\n",
            "Epoch 2 Batch 349 Loss 1.6122\n",
            "Epoch 2 Batch 350 Loss 1.4751\n",
            "Epoch 2 Batch 351 Loss 1.7159\n",
            "Epoch 2 Batch 352 Loss 1.6507\n",
            "Epoch 2 Batch 353 Loss 1.5402\n",
            "Epoch 2 Batch 354 Loss 1.6955\n",
            "Epoch 2 Batch 355 Loss 1.4377\n",
            "Epoch 2 Batch 356 Loss 1.5871\n",
            "Epoch 2 Batch 357 Loss 1.5167\n",
            "Epoch 2 Batch 358 Loss 1.4117\n",
            "Epoch 2 Batch 359 Loss 1.4070\n",
            "Epoch 2 Batch 360 Loss 1.2068\n",
            "Epoch 2 Batch 361 Loss 1.5129\n",
            "Epoch 2 Batch 362 Loss 1.4714\n",
            "Epoch 2 Batch 363 Loss 1.4739\n",
            "Epoch 2 Batch 364 Loss 1.5780\n",
            "Epoch 2 Batch 365 Loss 1.5680\n",
            "Epoch 2 Batch 366 Loss 1.4052\n",
            "Epoch 2 Batch 367 Loss 1.6990\n",
            "Epoch 2 Batch 368 Loss 1.4086\n",
            "Epoch 2 Batch 369 Loss 1.4059\n",
            "Epoch 2 Batch 370 Loss 1.3892\n",
            "Epoch 2 Batch 371 Loss 1.3765\n",
            "Epoch 2 Batch 372 Loss 1.4449\n",
            "Epoch 2 Batch 373 Loss 1.4846\n",
            "Epoch 2 Batch 374 Loss 1.6291\n",
            "Epoch 2 Batch 375 Loss 1.3332\n",
            "Epoch 2 Batch 376 Loss 1.6540\n",
            "Epoch 2 Batch 377 Loss 1.5419\n",
            "Epoch 2 Batch 378 Loss 1.5469\n",
            "Epoch 2 Batch 379 Loss 1.5395\n",
            "Epoch 2 Batch 380 Loss 1.7056\n",
            "Epoch 2 Batch 381 Loss 1.5146\n",
            "Epoch 2 Batch 382 Loss 1.5953\n",
            "Epoch 2 Batch 383 Loss 1.3542\n",
            "Epoch 2 Batch 384 Loss 1.6781\n",
            "Epoch 2 Batch 385 Loss 1.3701\n",
            "Epoch 2 Batch 386 Loss 1.4433\n",
            "Epoch 2 Batch 387 Loss 1.3853\n",
            "Epoch 2 Batch 388 Loss 1.7237\n",
            "Epoch 2 Batch 389 Loss 1.6463\n",
            "Epoch 2 Batch 390 Loss 1.2836\n",
            "Epoch 2 Batch 391 Loss 1.2600\n",
            "Epoch 2 Batch 392 Loss 1.5390\n",
            "Epoch 2 Batch 393 Loss 1.5470\n",
            "Epoch 2 Batch 394 Loss 1.5274\n",
            "Epoch 2 Batch 395 Loss 1.4497\n",
            "Epoch 2 Batch 396 Loss 1.6454\n",
            "Epoch 2 Batch 397 Loss 1.5164\n",
            "Epoch 2 Batch 398 Loss 1.2868\n",
            "Epoch 2 Batch 399 Loss 1.4596\n",
            "Epoch 2 Batch 400 Loss 1.3691\n",
            "Epoch 2 Batch 401 Loss 1.4316\n",
            "Epoch 2 Batch 402 Loss 1.1163\n",
            "Epoch 2 Batch 403 Loss 1.4333\n",
            "Epoch 2 Batch 404 Loss 1.4808\n",
            "Epoch 2 Batch 405 Loss 1.2970\n",
            "Epoch 2 Batch 406 Loss 1.2855\n",
            "Epoch 2 Batch 407 Loss 1.5232\n",
            "Epoch 2 Batch 408 Loss 1.4225\n",
            "Epoch 2 Batch 409 Loss 1.5119\n",
            "Epoch 2 Batch 410 Loss 1.4981\n",
            "Epoch 2 Batch 411 Loss 1.3175\n",
            "Epoch 2 Batch 412 Loss 1.5347\n",
            "Epoch 2 Batch 413 Loss 1.5565\n",
            "Epoch 2 Batch 414 Loss 1.3092\n",
            "Epoch 2 Batch 415 Loss 1.5049\n",
            "Epoch 2 Batch 416 Loss 1.5288\n",
            "Epoch 2 Batch 417 Loss 1.2907\n",
            "Epoch 2 Batch 418 Loss 1.7795\n",
            "Epoch 2 Batch 419 Loss 1.4989\n",
            "Epoch 2 Batch 420 Loss 1.4561\n",
            "Epoch 2 Batch 421 Loss 1.5868\n",
            "Epoch 2 Batch 422 Loss 1.3122\n",
            "Epoch 2 Batch 423 Loss 1.3504\n",
            "Epoch 2 Batch 424 Loss 1.3988\n",
            "Epoch 2 Batch 425 Loss 1.3013\n",
            "Epoch 2 Batch 426 Loss 1.4229\n",
            "Epoch 2 Batch 427 Loss 1.4187\n",
            "Epoch 2 Batch 428 Loss 1.5099\n",
            "Epoch 2 Batch 429 Loss 1.2982\n",
            "Epoch 2 Batch 430 Loss 1.3996\n",
            "Epoch 2 Batch 431 Loss 1.5272\n",
            "Epoch 2 Batch 432 Loss 1.6515\n",
            "Epoch 2 Batch 433 Loss 1.5112\n",
            "Epoch 2 Batch 434 Loss 1.5653\n",
            "Epoch 2 Batch 435 Loss 1.4036\n",
            "Epoch 2 Batch 436 Loss 1.4536\n",
            "Epoch 2 Batch 437 Loss 1.5502\n",
            "Epoch 2 Batch 438 Loss 1.3962\n",
            "Epoch 2 Batch 439 Loss 1.4792\n",
            "Epoch 2 Batch 440 Loss 1.5167\n",
            "Epoch 2 Batch 441 Loss 1.4924\n",
            "Epoch 2 Batch 442 Loss 1.6273\n",
            "Epoch 2 Batch 443 Loss 1.5769\n",
            "Epoch 2 Batch 444 Loss 1.4597\n",
            "Epoch 2 Batch 445 Loss 1.5034\n",
            "Epoch 2 Batch 446 Loss 1.2871\n",
            "Epoch 2 Batch 447 Loss 1.4956\n",
            "Epoch 2 Batch 448 Loss 1.4025\n",
            "Epoch 2 Batch 449 Loss 1.4391\n",
            "Epoch 2 Batch 450 Loss 1.1713\n",
            "Epoch 2 Batch 451 Loss 1.5301\n",
            "Epoch 2 Batch 452 Loss 1.4516\n",
            "Epoch 2 Batch 453 Loss 1.4736\n",
            "Epoch 2 Batch 454 Loss 1.3024\n",
            "Epoch 2 Batch 455 Loss 1.2681\n",
            "Epoch 2 Batch 456 Loss 1.4706\n",
            "Epoch 2 Batch 457 Loss 1.5944\n",
            "Epoch 2 Batch 458 Loss 1.2656\n",
            "Epoch 2 Batch 459 Loss 1.3341\n",
            "Epoch 2 Batch 460 Loss 1.6761\n",
            "Epoch 2 Batch 461 Loss 1.4872\n",
            "Epoch 2 Batch 462 Loss 1.6626\n",
            "Epoch 2 Batch 463 Loss 1.2217\n",
            "Epoch 2 Batch 464 Loss 1.3699\n",
            "Epoch 2 Batch 465 Loss 1.4861\n",
            "Epoch 2 Batch 466 Loss 1.5468\n",
            "Epoch 2 Batch 467 Loss 1.6134\n",
            "Epoch 2 Batch 468 Loss 1.4513\n",
            "Epoch 2 Batch 469 Loss 1.2111\n",
            "Epoch 2 Batch 470 Loss 1.4762\n",
            "Epoch 2 Batch 471 Loss 1.5239\n",
            "Epoch 2 Batch 472 Loss 1.3647\n",
            "Epoch 2 Batch 473 Loss 1.3741\n",
            "Epoch 2 Batch 474 Loss 1.4426\n",
            "Epoch 2 Batch 475 Loss 1.5581\n",
            "Epoch 2 Batch 476 Loss 1.4053\n",
            "Epoch 2 Batch 477 Loss 1.5321\n",
            "Epoch 2 Batch 478 Loss 1.6573\n",
            "Epoch 2 Batch 479 Loss 1.2268\n",
            "Epoch 2 Batch 480 Loss 1.3111\n",
            "Epoch 2 Batch 481 Loss 1.4518\n",
            "Epoch 2 Batch 482 Loss 1.4876\n",
            "Epoch 2 Batch 483 Loss 1.3564\n",
            "Epoch 2 Batch 484 Loss 1.6699\n",
            "Epoch 2 Batch 485 Loss 1.3489\n",
            "Epoch 2 Batch 486 Loss 1.6733\n",
            "Epoch 2 Batch 487 Loss 1.3549\n",
            "Epoch 2 Batch 488 Loss 1.4421\n",
            "Epoch 2 Batch 489 Loss 1.3886\n",
            "Epoch 2 Batch 490 Loss 1.4591\n",
            "Epoch 2 Batch 491 Loss 1.4942\n",
            "Epoch 2 Batch 492 Loss 1.5236\n",
            "Epoch 2 Batch 493 Loss 1.3364\n",
            "Epoch 2 Batch 494 Loss 1.3757\n",
            "Epoch 2 Batch 495 Loss 1.4429\n",
            "Epoch 2 Batch 496 Loss 1.4456\n",
            "Epoch 2 Batch 497 Loss 1.5327\n",
            "Epoch 2 Batch 498 Loss 1.6606\n",
            "Epoch 2 Batch 499 Loss 1.3155\n",
            "Epoch 2 Batch 500 Loss 1.4493\n",
            "Epoch 2 Batch 501 Loss 1.3071\n",
            "Epoch 2 Batch 502 Loss 1.4381\n",
            "Epoch 2 Batch 503 Loss 1.4628\n",
            "Epoch 2 Batch 504 Loss 1.6430\n",
            "Epoch 2 Batch 505 Loss 1.6568\n",
            "Epoch 2 Batch 506 Loss 1.3667\n",
            "Epoch 2 Batch 507 Loss 1.3559\n",
            "Epoch 2 Batch 508 Loss 1.3248\n",
            "Epoch 2 Batch 509 Loss 1.7117\n",
            "Epoch 2 Batch 510 Loss 1.5716\n",
            "Epoch 2 Batch 511 Loss 1.3806\n",
            "Epoch 2 Batch 512 Loss 1.3507\n",
            "Epoch 2 Batch 513 Loss 1.3304\n",
            "Epoch 2 Batch 514 Loss 1.5637\n",
            "Epoch 2 Batch 515 Loss 1.7639\n",
            "Epoch 2 Batch 516 Loss 1.3171\n",
            "Epoch 2 Batch 517 Loss 1.4367\n",
            "Epoch 2 Batch 518 Loss 1.4641\n",
            "Epoch 2 Batch 519 Loss 1.4303\n",
            "Epoch 2 Batch 520 Loss 1.4127\n",
            "Epoch 2 Batch 521 Loss 1.5943\n",
            "Epoch 2 Batch 522 Loss 1.2924\n",
            "Epoch 2 Batch 523 Loss 1.5026\n",
            "Epoch 2 Batch 524 Loss 1.3929\n",
            "Epoch 2 Batch 525 Loss 1.5643\n",
            "Epoch 2 Batch 526 Loss 1.4983\n",
            "Epoch 2 Batch 527 Loss 1.5543\n",
            "Epoch 2 Batch 528 Loss 1.5684\n",
            "Epoch 2 Batch 529 Loss 1.4577\n",
            "Epoch 2 Batch 530 Loss 1.2842\n",
            "Epoch 2 Batch 531 Loss 1.2422\n",
            "Epoch 2 Batch 532 Loss 1.6316\n",
            "Epoch 2 Batch 533 Loss 1.4390\n",
            "Epoch 2 Batch 534 Loss 1.5406\n",
            "Epoch 2 Batch 535 Loss 1.4476\n",
            "Epoch 2 Batch 536 Loss 1.3577\n",
            "Epoch 2 Batch 537 Loss 1.5101\n",
            "Epoch 2 Batch 538 Loss 1.4154\n",
            "Epoch 2 Batch 539 Loss 1.3448\n",
            "Epoch 2 Batch 540 Loss 1.3230\n",
            "Epoch 2 Batch 541 Loss 1.2067\n",
            "Epoch 2 Batch 542 Loss 1.2961\n",
            "Epoch 2 Batch 543 Loss 1.5310\n",
            "Epoch 2 Batch 544 Loss 1.7944\n",
            "Epoch 2 Batch 545 Loss 1.4452\n",
            "Epoch 2 Batch 546 Loss 1.3703\n",
            "Epoch 2 Batch 547 Loss 1.6455\n",
            "Epoch 2 Batch 548 Loss 1.3001\n",
            "Epoch 2 Batch 549 Loss 1.4395\n",
            "Epoch 2 Batch 550 Loss 1.5790\n",
            "Epoch 2 Batch 551 Loss 1.3393\n",
            "Epoch 2 Batch 552 Loss 1.4238\n",
            "Epoch 2 Batch 553 Loss 1.5297\n",
            "Epoch 2 Batch 554 Loss 1.5830\n",
            "Epoch 2 Batch 555 Loss 1.4722\n",
            "Epoch 2 Batch 556 Loss 1.3363\n",
            "Epoch 2 Batch 557 Loss 1.3227\n",
            "Epoch 2 Batch 558 Loss 1.4742\n",
            "Epoch 2 Batch 559 Loss 1.2169\n",
            "Epoch 2 Batch 560 Loss 1.7428\n",
            "Epoch 2 Batch 561 Loss 1.3817\n",
            "Epoch 2 Batch 562 Loss 1.5030\n",
            "Epoch 2 Batch 563 Loss 1.4671\n",
            "Epoch 2 Batch 564 Loss 1.4969\n",
            "Epoch 2 Batch 565 Loss 1.3998\n",
            "Epoch 2 Batch 566 Loss 1.5539\n",
            "Epoch 2 Batch 567 Loss 1.5794\n",
            "Epoch 2 Batch 568 Loss 1.4432\n",
            "Epoch 2 Batch 569 Loss 1.4683\n",
            "Epoch 2 Batch 570 Loss 1.3798\n",
            "Epoch 2 Batch 571 Loss 1.7326\n",
            "Epoch 2 Batch 572 Loss 1.5513\n",
            "Epoch 2 Batch 573 Loss 1.4549\n",
            "Epoch 2 Batch 574 Loss 1.3932\n",
            "Epoch 2 Batch 575 Loss 1.4210\n",
            "Epoch 2 Batch 576 Loss 1.4259\n",
            "Epoch 2 Batch 577 Loss 1.8075\n",
            "Epoch 2 Batch 578 Loss 1.3653\n",
            "Epoch 2 Batch 579 Loss 1.3037\n",
            "Epoch 2 Batch 580 Loss 1.3863\n",
            "Epoch 2 Batch 581 Loss 1.4069\n",
            "Epoch 2 Batch 582 Loss 1.6268\n",
            "Epoch 2 Batch 583 Loss 1.6363\n",
            "Epoch 2 Batch 584 Loss 1.5772\n",
            "Epoch 2 Batch 585 Loss 1.3614\n",
            "Epoch 2 Batch 586 Loss 1.2620\n",
            "Epoch 2 Batch 587 Loss 1.5089\n",
            "Epoch 2 Batch 588 Loss 1.1804\n",
            "Epoch 2 Batch 589 Loss 1.3981\n",
            "Epoch 2 Batch 590 Loss 1.2929\n",
            "Epoch 2 Batch 591 Loss 1.3037\n",
            "Epoch 2 Batch 592 Loss 1.4781\n",
            "Epoch 2 Batch 593 Loss 1.4641\n",
            "Epoch 2 Batch 594 Loss 1.4884\n",
            "Epoch 2 Batch 595 Loss 1.6655\n",
            "Epoch 2 Batch 596 Loss 1.3107\n",
            "Epoch 2 Batch 597 Loss 1.2630\n",
            "Epoch 2 Batch 598 Loss 1.2858\n",
            "Epoch 2 Batch 599 Loss 1.5996\n",
            "Epoch 2 Batch 600 Loss 1.4689\n",
            "Epoch 2 Batch 601 Loss 1.5511\n",
            "Epoch 2 Batch 602 Loss 1.3715\n",
            "Epoch 2 Batch 603 Loss 1.2589\n",
            "Epoch 2 Batch 604 Loss 1.3520\n",
            "Epoch 2 Batch 605 Loss 1.2916\n",
            "Epoch 2 Batch 606 Loss 1.5892\n",
            "Epoch 2 Batch 607 Loss 1.5270\n",
            "Epoch 2 Batch 608 Loss 1.4720\n",
            "Epoch 2 Batch 609 Loss 1.3522\n",
            "Epoch 2 Batch 610 Loss 1.4924\n",
            "Epoch 2 Batch 611 Loss 1.2962\n",
            "Epoch 2 Batch 612 Loss 1.6515\n",
            "Epoch 2 Batch 613 Loss 1.5054\n",
            "Epoch 2 Batch 614 Loss 1.8550\n",
            "Epoch 2 Batch 615 Loss 1.4155\n",
            "Epoch 2 Batch 616 Loss 1.3768\n",
            "Epoch 2 Batch 617 Loss 1.4091\n",
            "Epoch 2 Batch 618 Loss 1.3444\n",
            "Epoch 2 Batch 619 Loss 1.4217\n",
            "Epoch 2 Batch 620 Loss 1.6069\n",
            "Epoch 2 Batch 621 Loss 1.4687\n",
            "Epoch 2 Batch 622 Loss 1.3630\n",
            "Epoch 2 Batch 623 Loss 1.4228\n",
            "Epoch 2 Batch 624 Loss 1.4822\n",
            "Epoch 2 Batch 625 Loss 1.4687\n",
            "Epoch 2 Batch 626 Loss 1.4751\n",
            "Epoch 2 Batch 627 Loss 1.3795\n",
            "Epoch 2 Batch 628 Loss 1.2909\n",
            "Epoch 2 Batch 629 Loss 1.2928\n",
            "Epoch 2 Batch 630 Loss 1.5359\n",
            "Epoch 2 Batch 631 Loss 1.4338\n",
            "Epoch 2 Batch 632 Loss 1.3939\n",
            "Epoch 2 Batch 633 Loss 1.5091\n",
            "Epoch 2 Batch 634 Loss 1.3926\n",
            "Epoch 2 Batch 635 Loss 1.6539\n",
            "Epoch 2 Batch 636 Loss 1.4706\n",
            "Epoch 2 Batch 637 Loss 1.3645\n",
            "Epoch 2 Batch 638 Loss 1.4002\n",
            "Epoch 2 Batch 639 Loss 1.3543\n",
            "Epoch 2 Batch 640 Loss 1.5006\n",
            "Epoch 2 Batch 641 Loss 1.7304\n",
            "Epoch 2 Batch 642 Loss 1.5149\n",
            "Epoch 2 Batch 643 Loss 1.6336\n",
            "Epoch 2 Batch 644 Loss 1.3214\n",
            "Epoch 2 Batch 645 Loss 1.5450\n",
            "Epoch 2 Batch 646 Loss 1.5648\n",
            "Epoch 2 Batch 647 Loss 1.5596\n",
            "Epoch 2 Batch 648 Loss 1.3255\n",
            "Epoch 2 Batch 649 Loss 1.5345\n",
            "Epoch 2 Batch 650 Loss 1.5019\n",
            "Epoch 2 Batch 651 Loss 1.5191\n",
            "Epoch 2 Batch 652 Loss 1.4201\n",
            "Epoch 2 Batch 653 Loss 1.2706\n",
            "Epoch 2 Batch 654 Loss 1.4626\n",
            "Epoch 2 Batch 655 Loss 1.3221\n",
            "Epoch 2 Batch 656 Loss 1.5077\n",
            "Epoch 2 Batch 657 Loss 1.3076\n",
            "Epoch 2 Batch 658 Loss 1.2834\n",
            "Epoch 2 Batch 659 Loss 1.4159\n",
            "Epoch 2 Batch 660 Loss 1.5191\n",
            "Epoch 2 Batch 661 Loss 1.5824\n",
            "Epoch 2 Batch 662 Loss 1.2942\n",
            "Epoch 2 Batch 663 Loss 1.5034\n",
            "Epoch 2 Batch 664 Loss 1.6267\n",
            "Epoch 2 Batch 665 Loss 1.4064\n",
            "Epoch 2 Batch 666 Loss 1.5043\n",
            "Epoch 2 Batch 667 Loss 1.4166\n",
            "Epoch 2 Batch 668 Loss 1.5141\n",
            "Epoch 2 Batch 669 Loss 1.3588\n",
            "Epoch 2 Batch 670 Loss 1.4521\n",
            "Epoch 2 Batch 671 Loss 1.5045\n",
            "Epoch 2 Batch 672 Loss 1.2776\n",
            "Epoch 2 Batch 673 Loss 1.6523\n",
            "Epoch 2 Batch 674 Loss 1.3106\n",
            "Epoch 2 Batch 675 Loss 1.4252\n",
            "Epoch 2 Batch 676 Loss 1.5173\n",
            "Epoch 2 Batch 677 Loss 1.5364\n",
            "Epoch 2 Batch 678 Loss 1.3388\n",
            "Epoch 2 Batch 679 Loss 1.3520\n",
            "Epoch 2 Batch 680 Loss 1.4235\n",
            "Epoch 2 Batch 681 Loss 1.7228\n",
            "Epoch 2 Batch 682 Loss 1.6421\n",
            "Epoch 2 Batch 683 Loss 1.3896\n",
            "Epoch 2 Batch 684 Loss 1.3154\n",
            "Epoch 2 Batch 685 Loss 1.3291\n",
            "Epoch 2 Batch 686 Loss 1.4371\n",
            "Epoch 2 Batch 687 Loss 1.2429\n",
            "Epoch 2 Batch 688 Loss 1.4616\n",
            "Epoch 2 Batch 689 Loss 1.5422\n",
            "Epoch 2 Batch 690 Loss 1.3464\n",
            "Epoch 2 Batch 691 Loss 1.3041\n",
            "Epoch 2 Batch 692 Loss 1.8138\n",
            "Epoch 2 Batch 693 Loss 1.3590\n",
            "Epoch 2 Batch 694 Loss 1.5557\n",
            "Epoch 2 Batch 695 Loss 1.4692\n",
            "Epoch 2 Batch 696 Loss 1.5483\n",
            "Epoch 2 Batch 697 Loss 1.2760\n",
            "Epoch 2 Batch 698 Loss 1.4806\n",
            "Epoch 2 Batch 699 Loss 1.4970\n",
            "Epoch 2 Batch 700 Loss 1.4690\n",
            "Epoch 2 Batch 701 Loss 1.6571\n",
            "Epoch 2 Batch 702 Loss 1.3357\n",
            "Epoch 2 Batch 703 Loss 1.6217\n",
            "Epoch 2 Batch 704 Loss 1.5469\n",
            "Epoch 2 Batch 705 Loss 1.3082\n",
            "Epoch 2 Batch 706 Loss 1.4906\n",
            "Epoch 2 Batch 707 Loss 1.4226\n",
            "Epoch 2 Batch 708 Loss 1.4558\n",
            "Epoch 2 Batch 709 Loss 1.3862\n",
            "Epoch 2 Batch 710 Loss 1.6158\n",
            "Epoch 2 Batch 711 Loss 1.3750\n",
            "Epoch 2 Batch 712 Loss 1.4136\n",
            "Epoch 2 Batch 713 Loss 1.1398\n",
            "Epoch 2 Batch 714 Loss 1.4223\n",
            "Epoch 2 Batch 715 Loss 1.5437\n",
            "Epoch 2 Batch 716 Loss 1.4516\n",
            "Epoch 2 Batch 717 Loss 1.5635\n",
            "Epoch 2 Batch 718 Loss 1.3496\n",
            "Epoch 2 Batch 719 Loss 1.5421\n",
            "Epoch 2 Batch 720 Loss 1.4533\n",
            "Epoch 2 Batch 721 Loss 1.3996\n",
            "Epoch 2 Batch 722 Loss 1.1755\n",
            "Epoch 2 Batch 723 Loss 1.5695\n",
            "Epoch 2 Batch 724 Loss 1.3519\n",
            "Epoch 2 Batch 725 Loss 1.5085\n",
            "Epoch 2 Batch 726 Loss 1.4466\n",
            "Epoch 2 Batch 727 Loss 1.2853\n",
            "Epoch 2 Batch 728 Loss 1.1968\n",
            "Epoch 2 Batch 729 Loss 1.4889\n",
            "Epoch 2 Batch 730 Loss 1.6389\n",
            "Epoch 2 Batch 731 Loss 1.4702\n",
            "Epoch 2 Batch 732 Loss 1.5920\n",
            "Epoch 2 Batch 733 Loss 1.4360\n",
            "Epoch 2 Batch 734 Loss 1.1423\n",
            "Epoch 2 Batch 735 Loss 1.4041\n",
            "Epoch 2 Batch 736 Loss 1.2618\n",
            "Epoch 2 Batch 737 Loss 1.3855\n",
            "Epoch 2 Batch 738 Loss 1.3613\n",
            "Epoch 2 Batch 739 Loss 1.3451\n",
            "Epoch 2 Batch 740 Loss 1.4665\n",
            "Epoch 2 Batch 741 Loss 1.5591\n",
            "Epoch 2 Batch 742 Loss 1.3498\n",
            "Epoch 2 Batch 743 Loss 1.3724\n",
            "Epoch 2 Batch 744 Loss 1.4228\n",
            "Epoch 2 Batch 745 Loss 1.5109\n",
            "Epoch 2 Batch 746 Loss 1.3972\n",
            "Epoch 2 Batch 747 Loss 1.2678\n",
            "Epoch 2 Batch 748 Loss 1.5724\n",
            "Epoch 2 Batch 749 Loss 1.2883\n",
            "Epoch 2 Batch 750 Loss 1.6451\n",
            "Epoch 2 Batch 751 Loss 1.2254\n",
            "Epoch 2 Batch 752 Loss 1.5145\n",
            "Epoch 2 Batch 753 Loss 1.3338\n",
            "Epoch 2 Batch 754 Loss 1.2698\n",
            "Epoch 2 Batch 755 Loss 1.5885\n",
            "Epoch 2 Batch 756 Loss 1.4007\n",
            "Epoch 2 Batch 757 Loss 1.4300\n",
            "Epoch 2 Batch 758 Loss 1.6149\n",
            "Epoch 2 Batch 759 Loss 1.3835\n",
            "Epoch 2 Batch 760 Loss 1.4155\n",
            "Epoch 2 Batch 761 Loss 1.4990\n",
            "Epoch 2 Batch 762 Loss 1.4494\n",
            "Epoch 2 Batch 763 Loss 1.4034\n",
            "Epoch 2 Batch 764 Loss 1.4328\n",
            "Epoch 2 Batch 765 Loss 1.5072\n",
            "Epoch 2 Batch 766 Loss 1.2459\n",
            "Epoch 2 Batch 767 Loss 1.4740\n",
            "Epoch 2 Batch 768 Loss 1.2214\n",
            "Epoch 2 Batch 769 Loss 1.4080\n",
            "Epoch 2 Batch 770 Loss 1.6146\n",
            "Epoch 2 Batch 771 Loss 1.4553\n",
            "Epoch 2 Batch 772 Loss 1.3865\n",
            "Epoch 2 Batch 773 Loss 1.4257\n",
            "Epoch 2 Batch 774 Loss 1.6272\n",
            "Epoch 2 Batch 775 Loss 1.3451\n",
            "Epoch 2 Batch 776 Loss 1.3188\n",
            "Epoch 2 Batch 777 Loss 1.4700\n",
            "Epoch 2 Batch 778 Loss 1.4423\n",
            "Epoch 2 Batch 779 Loss 1.7344\n",
            "Epoch 2 Batch 780 Loss 1.5094\n",
            "Epoch 2 Batch 781 Loss 1.6907\n",
            "Epoch 2 Batch 782 Loss 1.4217\n",
            "Epoch 2 Batch 783 Loss 1.5168\n",
            "Epoch 2 Batch 784 Loss 1.4073\n",
            "Epoch 2 Batch 785 Loss 1.3595\n",
            "Epoch 2 Batch 786 Loss 1.4494\n",
            "Epoch 2 Batch 787 Loss 1.3496\n",
            "Epoch 2 Batch 788 Loss 1.3232\n",
            "Epoch 2 Batch 789 Loss 1.2859\n",
            "Epoch 2 Batch 790 Loss 1.4934\n",
            "Epoch 2 Batch 791 Loss 1.4828\n",
            "Epoch 2 Batch 792 Loss 1.5036\n",
            "Epoch 2 Batch 793 Loss 1.2558\n",
            "Epoch 2 Batch 794 Loss 1.3939\n",
            "Epoch 2 Batch 795 Loss 1.7314\n",
            "Epoch 2 Batch 796 Loss 1.5165\n",
            "Epoch 2 Batch 797 Loss 1.3133\n",
            "Epoch 2 Batch 798 Loss 1.5608\n",
            "Epoch 2 Batch 799 Loss 1.4198\n",
            "Epoch 2 Batch 800 Loss 1.4210\n",
            "Epoch 2 Batch 801 Loss 1.4833\n",
            "Epoch 2 Batch 802 Loss 1.1794\n",
            "Epoch 2 Batch 803 Loss 1.3683\n",
            "Epoch 2 Batch 804 Loss 1.4512\n",
            "Epoch 2 Batch 805 Loss 1.5460\n",
            "Epoch 2 Batch 806 Loss 1.5006\n",
            "Epoch 2 Batch 807 Loss 1.3630\n",
            "Epoch 2 Batch 808 Loss 1.1857\n",
            "Epoch 2 Batch 809 Loss 1.3838\n",
            "Epoch 2 Batch 810 Loss 1.5691\n",
            "Epoch 2 Batch 811 Loss 1.3744\n",
            "Epoch 2 Batch 812 Loss 1.2641\n",
            "Epoch 2 Batch 813 Loss 1.5932\n",
            "Epoch 2 Batch 814 Loss 1.3731\n",
            "Epoch 2 Batch 815 Loss 1.2787\n",
            "Epoch 2 Batch 816 Loss 1.6092\n",
            "Epoch 2 Batch 817 Loss 1.4999\n",
            "Epoch 2 Batch 818 Loss 1.3677\n",
            "Epoch 2 Batch 819 Loss 1.5350\n",
            "Epoch 2 Batch 820 Loss 1.3755\n",
            "Epoch 2 Batch 821 Loss 1.5230\n",
            "Epoch 2 Batch 822 Loss 1.5120\n",
            "Epoch 2 Batch 823 Loss 1.3142\n",
            "Epoch 2 Batch 824 Loss 1.6073\n",
            "Epoch 2 Batch 825 Loss 1.1432\n",
            "Epoch 2 Batch 826 Loss 1.5352\n",
            "Epoch 2 Batch 827 Loss 1.3295\n",
            "Epoch 2 Batch 828 Loss 1.5030\n",
            "Epoch 2 Batch 829 Loss 1.4409\n",
            "Epoch 2 Batch 830 Loss 1.1630\n",
            "Epoch 2 Batch 831 Loss 1.5547\n",
            "Epoch 2 Batch 832 Loss 1.3483\n",
            "Epoch 2 Batch 833 Loss 1.3750\n",
            "Epoch 2 Batch 834 Loss 1.0707\n",
            "Epoch 2 Batch 835 Loss 1.3148\n",
            "Epoch 2 Batch 836 Loss 1.4141\n",
            "Epoch 2 Batch 837 Loss 1.4236\n",
            "Epoch 2 Batch 838 Loss 1.4542\n",
            "Epoch 2 Batch 839 Loss 1.2817\n",
            "Epoch 2 Batch 840 Loss 1.3925\n",
            "Epoch 2 Batch 841 Loss 1.5551\n",
            "Epoch 2 Batch 842 Loss 1.5197\n",
            "Epoch 2 Batch 843 Loss 1.6998\n",
            "Epoch 2 Batch 844 Loss 1.3455\n",
            "Epoch 2 Batch 845 Loss 1.6598\n",
            "Epoch 2 Batch 846 Loss 1.5074\n",
            "Epoch 2 Batch 847 Loss 1.5512\n",
            "Epoch 2 Batch 848 Loss 1.3393\n",
            "Epoch 2 Batch 849 Loss 1.4077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentence(sentence):\n",
        "    '''\n",
        "    特殊符号去除\n",
        "    :param sentence: 待处理的字符串\n",
        "    :return: 过滤特殊字符后的字符串\n",
        "    '''\n",
        "    if isinstance(sentence, str):\n",
        "        return re.sub(\n",
        "            r'[\\s+\\-\\/\\[\\]\\{\\}_$%^*(+\\\"\\')]+|[+——()【】“”~@#￥%……&*（）]+|你好,|您好,|你好，|您好，',\n",
        "            # r'[\\s+\\-\\!\\/\\[\\]\\{\\}_,.$%^*(+\\\"\\')]+|[:：+——()?【】“”！，。？、~@#￥%……&*（）]+|车主说|技师说|语音|图片|你好|您好',\n",
        "            ' ', sentence)\n",
        "    else:\n",
        "        return ' '"
      ],
      "metadata": {
        "id": "hWzNiRpVqWEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seg_proc(sentence):\n",
        "    tokens = sentence.split('|')\n",
        "    result = []\n",
        "    for t in tokens:\n",
        "        result.append(cut_sentence(t))\n",
        "    return ' | '.join(result)"
      ],
      "metadata": {
        "id": "_h18MBRpqz3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "def cut_sentence(line):\n",
        "    # 切词，默认精确模式，全模式cut参数cut_all=True\n",
        "    tokens = jieba.cut(line)\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "Hnok1E3xr5SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d7oLRfPnt2DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_words = ['|', '[', ']', '语音', '图片']\n",
        "def filter_words(sentence):\n",
        "    '''\n",
        "    过滤停用词\n",
        "    :param seg_list: 切好词的列表 [word1 ,word2 .......]\n",
        "    :return: 过滤后的停用词\n",
        "    '''\n",
        "    words = sentence.split(' ')\n",
        "    # 去掉多余空字符\n",
        "    words = [word for word in words if word and word not in remove_words]\n",
        "    # 去掉停用词 包括一下标点符号也会去掉\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "cCkn4ytDr0Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_proc(sentence):\n",
        "    '''\n",
        "    预处理模块\n",
        "    :param sentence:待处理字符串\n",
        "    :return: 处理后的字符串\n",
        "    '''\n",
        "    # 清除无用词\n",
        "    sentence = clean_sentence(sentence)\n",
        "    # 分段切词\n",
        "    sentence = seg_proc(sentence)\n",
        "    # 过滤停用词\n",
        "    words = filter_words(sentence)\n",
        "    # 拼接成一个字符串,按空格分隔\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "-Tn5tfcXoL0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_proc(sentence, max_len, vocab):\n",
        "    '''\n",
        "    # 填充字段\n",
        "    < start > < end > < pad > < unk > max_lens\n",
        "    '''\n",
        "    # 0.按空格统计切分出词\n",
        "    words = sentence.strip().split(' ')\n",
        "    # 1. 截取规定长度的词数\n",
        "    words = words[:max_len]\n",
        "    # 2. 填充< unk > ,判断是否在vocab中, 不在填充 < unk >\n",
        "    sentence = [word if word in vocab else Vocab.UNKNOWN_TOKEN for word in words]\n",
        "    # 3. 填充< start > < end >\n",
        "    sentence = [Vocab.START_DECODING] + sentence + [Vocab.STOP_DECODING]\n",
        "    # 4. 判断长度，填充　< pad >\n",
        "    sentence = sentence + [Vocab.PAD_TOKEN] * (max_len - len(words))\n",
        "    return ' '.join(sentence)"
      ],
      "metadata": {
        "id": "kBb_UmfSoULu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(sentence, vocab):\n",
        "    \"\"\"\n",
        "    word 2 index\n",
        "    :param sentence: [word1,word2,word3, ...] ---> [index1,index2,index3 ......]\n",
        "    :param vocab: 词表\n",
        "    :return: 转换后的序列\n",
        "    \"\"\"\n",
        "    # 字符串切分成词\n",
        "    words = sentence.split(' ')\n",
        "    # 按照vocab的index进行转换         # 遇到未知词就填充unk的索引\n",
        "    ids = [vocab[word] if word in vocab else Vocab.UNKNOWN_TOKEN_INDEX for word in words]\n",
        "    return ids"
      ],
      "metadata": {
        "id": "h-8duwHCoYIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence, max_len, vocab):\n",
        "    \"\"\"\n",
        "    单句话预处理\n",
        "    \"\"\"\n",
        "    # 1. 切词处理\n",
        "    sentence = sentence_proc(sentence)\n",
        "    # 2. 填充\n",
        "    sentence = pad_proc(sentence, max_len - 2, vocab)\n",
        "    # 3. 转换index\n",
        "    sentence = transform_data(sentence, vocab)\n",
        "    return np.array([sentence])"
      ],
      "metadata": {
        "id": "9uC1VsoNnjeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp+2))\n",
        " \n",
        "    inputs = preprocess_sentence(sentence,max_length_inp,vocab)\n",
        " \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        " \n",
        "    result = ''\n",
        "    \n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        " \n",
        "    dec_hidden = enc_hidden\n",
        "    \n",
        "    dec_input = tf.expand_dims([vocab['<START>']], 0)\n",
        " \n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        \n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        \n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        " \n",
        "        result += reverse_vocab[predicted_id] + ' '\n",
        "        if reverse_vocab[predicted_id] == '<STOP>':\n",
        "            return result, sentence, attention_plot\n",
        " \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        " \n",
        "    return result, sentence, attention_plot\n",
        "\n"
      ],
      "metadata": {
        "id": "NsCjMMobkS8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='漏机油 具体 部位 发动机 变速器 正中间 位置 拍 中间 上面 上 已经 看见'\n",
        "\n",
        "translate(sentence)"
      ],
      "metadata": {
        "id": "_Y4m5tD0kUB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}